{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Head poses Model Training\n",
    "\n",
    "Neural Network model for a quick Head Pose estimation, using just facial points as input. The model was made for real-time control of a Pan-Tilt camera using face movements as the control signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies, set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(14)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Dropout, Activation, Conv2D, MaxPooling2D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import regularizers \n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 61)\n"
     ]
    }
   ],
   "source": [
    "dfCenter = pd.read_csv('datasets/face_center.csv')\n",
    "dfCenter2 = pd.read_csv('datasets/face_center2.csv')\n",
    "dfCenter = dfCenter.append(dfCenter2, ignore_index = True)\n",
    "\n",
    "dfRight = pd.read_csv('datasets/face_right.csv')\n",
    "dfRight2 = pd.read_csv('datasets/face_right2.csv')\n",
    "dfRight = dfRight.append(dfRight2, ignore_index = True)\n",
    "\n",
    "dfLeft = pd.read_csv('datasets/face_left.csv')\n",
    "dfLeft2 = pd.read_csv('datasets/face_left2.csv')\n",
    "dfLeft = dfLeft.append(dfLeft2, ignore_index = True)\n",
    "\n",
    "dfUp = pd.read_csv('datasets/face_up.csv')\n",
    "dfUp2 = pd.read_csv('datasets/face_up2.csv')\n",
    "dfUp = dfUp.append(dfUp2, ignore_index = True)\n",
    "\n",
    "dfDown = pd.read_csv('datasets/face_down.csv')\n",
    "dfDown2 = pd.read_csv('datasets/face_down2.csv')\n",
    "dfDown = dfDown.append(dfDown2, ignore_index = True)\n",
    "\n",
    "dfUpRight = pd.read_csv('datasets/face_up_right.csv')\n",
    "dfUpRight2 = pd.read_csv('datasets/face_up_right2.csv')\n",
    "dfUpRight = dfUpRight.append(dfUpRight2, ignore_index = True)\n",
    "\n",
    "dfUpLeft = pd.read_csv('datasets/face_up_left.csv')\n",
    "dfUpLeft2 = pd.read_csv('datasets/face_up_left2.csv')\n",
    "dfUpLeft = dfUpLeft.append(dfUpLeft2, ignore_index = True)\n",
    "\n",
    "dfDownRight = pd.read_csv('datasets/face_down_right.csv')\n",
    "dfDownRight2 = pd.read_csv('datasets/face_down_right2.csv')\n",
    "dfDownRight = dfDownRight.append(dfDownRight2, ignore_index = True)\n",
    "\n",
    "dfDownLeft = pd.read_csv('datasets/face_down_left.csv')\n",
    "dfDownLeft2 = pd.read_csv('datasets/face_down_left2.csv')\n",
    "dfDownLeft = dfDownLeft.append(dfDownLeft2, ignore_index = True)\n",
    "\n",
    "columns = list(dfCenter)\n",
    "\n",
    "dfCenter['RESULT'] = 0\n",
    "dfRight['RESULT'] = 1\n",
    "dfLeft['RESULT'] = 2\n",
    "dfUp['RESULT'] = 3\n",
    "dfDown['RESULT'] = 4\n",
    "dfUpRight['RESULT'] = 5\n",
    "dfUpLeft['RESULT'] = 6\n",
    "dfDownRight['RESULT'] = 7\n",
    "dfDownLeft['RESULT'] = 8\n",
    "\n",
    "\n",
    "df = dfCenter.append(dfRight, ignore_index=True).append(dfLeft, ignore_index = True)\n",
    "df = df.append(dfUp, ignore_index = True).append(dfDown, ignore_index = True)\n",
    "df = df.append(dfUpRight, ignore_index = True).append(dfUpLeft, ignore_index = True)\n",
    "df = df.append(dfDownRight, ignore_index = True).append(dfDownLeft, ignore_index = True)\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870639</td>\n",
       "      <td>0.778903</td>\n",
       "      <td>0.725278</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>0.661416</td>\n",
       "      <td>0.679687</td>\n",
       "      <td>0.725736</td>\n",
       "      <td>0.733129</td>\n",
       "      <td>0.697956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440451</td>\n",
       "      <td>0.461847</td>\n",
       "      <td>0.483469</td>\n",
       "      <td>0.492717</td>\n",
       "      <td>0.535039</td>\n",
       "      <td>0.545349</td>\n",
       "      <td>0.529865</td>\n",
       "      <td>0.501872</td>\n",
       "      <td>0.437781</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873671</td>\n",
       "      <td>0.781486</td>\n",
       "      <td>0.735074</td>\n",
       "      <td>0.683443</td>\n",
       "      <td>0.655249</td>\n",
       "      <td>0.673143</td>\n",
       "      <td>0.724177</td>\n",
       "      <td>0.729398</td>\n",
       "      <td>0.696364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465730</td>\n",
       "      <td>0.481957</td>\n",
       "      <td>0.495796</td>\n",
       "      <td>0.492139</td>\n",
       "      <td>0.528186</td>\n",
       "      <td>0.541326</td>\n",
       "      <td>0.529552</td>\n",
       "      <td>0.497978</td>\n",
       "      <td>0.431634</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877431</td>\n",
       "      <td>0.790890</td>\n",
       "      <td>0.744683</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>0.677805</td>\n",
       "      <td>0.718668</td>\n",
       "      <td>0.727266</td>\n",
       "      <td>0.696576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448371</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.488073</td>\n",
       "      <td>0.494257</td>\n",
       "      <td>0.521760</td>\n",
       "      <td>0.529427</td>\n",
       "      <td>0.515435</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.433165</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.792698</td>\n",
       "      <td>0.741344</td>\n",
       "      <td>0.697454</td>\n",
       "      <td>0.673473</td>\n",
       "      <td>0.686590</td>\n",
       "      <td>0.721158</td>\n",
       "      <td>0.718938</td>\n",
       "      <td>0.687107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426923</td>\n",
       "      <td>0.447417</td>\n",
       "      <td>0.465064</td>\n",
       "      <td>0.479192</td>\n",
       "      <td>0.519957</td>\n",
       "      <td>0.540319</td>\n",
       "      <td>0.528465</td>\n",
       "      <td>0.506383</td>\n",
       "      <td>0.439765</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.794662</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.701630</td>\n",
       "      <td>0.679918</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.731132</td>\n",
       "      <td>0.732642</td>\n",
       "      <td>0.692457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448210</td>\n",
       "      <td>0.467434</td>\n",
       "      <td>0.485004</td>\n",
       "      <td>0.482802</td>\n",
       "      <td>0.528204</td>\n",
       "      <td>0.543669</td>\n",
       "      <td>0.532557</td>\n",
       "      <td>0.512542</td>\n",
       "      <td>0.446091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6  \\\n",
       "8995  1.0  0.870639  0.778903  0.725278  0.680233  0.661416  0.679687   \n",
       "8996  1.0  0.873671  0.781486  0.735074  0.683443  0.655249  0.673143   \n",
       "8997  1.0  0.877431  0.790890  0.744683  0.692465  0.664408  0.677805   \n",
       "8998  1.0  0.881703  0.792698  0.741344  0.697454  0.673473  0.686590   \n",
       "8999  1.0  0.881703  0.794662  0.747706  0.701630  0.679918  0.693795   \n",
       "\n",
       "             7         8         9  ...        51        52        53  \\\n",
       "8995  0.725736  0.733129  0.697956  ...  0.440451  0.461847  0.483469   \n",
       "8996  0.724177  0.729398  0.696364  ...  0.465730  0.481957  0.495796   \n",
       "8997  0.718668  0.727266  0.696576  ...  0.448371  0.470152  0.488073   \n",
       "8998  0.721158  0.718938  0.687107  ...  0.426923  0.447417  0.465064   \n",
       "8999  0.731132  0.732642  0.692457  ...  0.448210  0.467434  0.485004   \n",
       "\n",
       "            54        55        56        57        58        59  RESULT  \n",
       "8995  0.492717  0.535039  0.545349  0.529865  0.501872  0.437781       8  \n",
       "8996  0.492139  0.528186  0.541326  0.529552  0.497978  0.431634       8  \n",
       "8997  0.494257  0.521760  0.529427  0.515435  0.493970  0.433165       8  \n",
       "8998  0.479192  0.519957  0.540319  0.528465  0.506383  0.439765       8  \n",
       "8999  0.482802  0.528204  0.543669  0.532557  0.512542  0.446091       8  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 61)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data and desired output, and then split training and test observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = df[columns]\n",
    "y = df['RESULT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=14)\n",
    "\n",
    "c = np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "\n",
    "n_classes = 9\n",
    "#one hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print (y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values \n",
    "Y_train_np = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X_train).astype('float32')\n",
    "Y = np.array(y_train)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 60)\n",
      "(1800, 9)\n",
      "(7200, 60, 1)\n",
      "(1800, 60, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7200, 9)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train_np = np.expand_dims(X_train_np, axis=2)\n",
    "print(X_train_np.shape)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "print(X_test.shape)\n",
    "#Y_train_np = np.expand_dims(Y_train_np, axis=2)\n",
    "Y_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(layer_sizes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(64, 3, padding ='same', input_shape=(60,1)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(128, 3, padding ='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(256, 3, padding ='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(256, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "  \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "  \n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(128, activation = 'elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(lr=0.00125),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate(model, batch_size=128, epochs=70):\n",
    "    model.summary()\n",
    "    history = model.fit(X_train_np, Y_train_np, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_119 (Conv1D)          (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 30, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 30, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 30, 128)           24704     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 15, 256)           98560     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_29  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 434,377\n",
      "Trainable params: 433,481\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/70\n",
      "7200/7200 [==============================] - 32s 4ms/step - loss: 1.4317 - acc: 0.5672 - val_loss: 1.2426 - val_acc: 0.6289\n",
      "Epoch 2/70\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.8926 - acc: 0.8049 - val_loss: 0.9150 - val_acc: 0.7789\n",
      "Epoch 3/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.6838 - acc: 0.8678 - val_loss: 0.7619 - val_acc: 0.8017\n",
      "Epoch 4/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.5768 - acc: 0.8971 - val_loss: 0.6104 - val_acc: 0.8333\n",
      "Epoch 5/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.5013 - acc: 0.9115 - val_loss: 0.5356 - val_acc: 0.8572\n",
      "Epoch 6/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4460 - acc: 0.9189 - val_loss: 0.4461 - val_acc: 0.8944\n",
      "Epoch 7/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.4022 - acc: 0.9264 - val_loss: 0.3914 - val_acc: 0.9228\n",
      "Epoch 8/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3645 - acc: 0.9344 - val_loss: 0.3311 - val_acc: 0.9389\n",
      "Epoch 9/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3396 - acc: 0.9353 - val_loss: 0.3024 - val_acc: 0.9450\n",
      "Epoch 10/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.3166 - acc: 0.9410 - val_loss: 0.2717 - val_acc: 0.9517\n",
      "Epoch 11/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2940 - acc: 0.9450 - val_loss: 0.2588 - val_acc: 0.9494\n",
      "Epoch 12/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.2793 - acc: 0.9442 - val_loss: 0.2415 - val_acc: 0.9528\n",
      "Epoch 13/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.2615 - acc: 0.9478 - val_loss: 0.2295 - val_acc: 0.9561\n",
      "Epoch 14/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.2517 - acc: 0.9519 - val_loss: 0.2155 - val_acc: 0.9589\n",
      "Epoch 15/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.2413 - acc: 0.9504 - val_loss: 0.2080 - val_acc: 0.9594\n",
      "Epoch 16/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.2312 - acc: 0.9528 - val_loss: 0.2016 - val_acc: 0.9572\n",
      "Epoch 17/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.2235 - acc: 0.9564 - val_loss: 0.1931 - val_acc: 0.9611\n",
      "Epoch 18/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.2128 - acc: 0.9551 - val_loss: 0.1868 - val_acc: 0.9639\n",
      "Epoch 19/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.2053 - acc: 0.9583 - val_loss: 0.1754 - val_acc: 0.9672\n",
      "Epoch 20/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2014 - acc: 0.9593 - val_loss: 0.1695 - val_acc: 0.9667\n",
      "Epoch 21/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1936 - acc: 0.9613 - val_loss: 0.1669 - val_acc: 0.9650\n",
      "Epoch 22/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1910 - acc: 0.9590 - val_loss: 0.1605 - val_acc: 0.9672\n",
      "Epoch 23/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1840 - acc: 0.9629 - val_loss: 0.1528 - val_acc: 0.9694\n",
      "Epoch 24/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1825 - acc: 0.9618 - val_loss: 0.1504 - val_acc: 0.9689\n",
      "Epoch 25/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1732 - acc: 0.9639 - val_loss: 0.1521 - val_acc: 0.9683\n",
      "Epoch 26/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.1705 - acc: 0.9643 - val_loss: 0.1472 - val_acc: 0.9689\n",
      "Epoch 27/70\n",
      "7200/7200 [==============================] - 22s 3ms/step - loss: 0.1674 - acc: 0.9643 - val_loss: 0.1498 - val_acc: 0.9661\n",
      "Epoch 28/70\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.1611 - acc: 0.9660 - val_loss: 0.1368 - val_acc: 0.9689\n",
      "Epoch 29/70\n",
      "7200/7200 [==============================] - 21s 3ms/step - loss: 0.1581 - acc: 0.9658 - val_loss: 0.1317 - val_acc: 0.9706\n",
      "Epoch 30/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1529 - acc: 0.9663 - val_loss: 0.1263 - val_acc: 0.9717\n",
      "Epoch 31/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1502 - acc: 0.9667 - val_loss: 0.1280 - val_acc: 0.9717\n",
      "Epoch 32/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1465 - acc: 0.9669 - val_loss: 0.1239 - val_acc: 0.9706\n",
      "Epoch 33/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1442 - acc: 0.9682 - val_loss: 0.1187 - val_acc: 0.9739\n",
      "Epoch 34/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1393 - acc: 0.9694 - val_loss: 0.1189 - val_acc: 0.9706\n",
      "Epoch 35/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1361 - acc: 0.9690 - val_loss: 0.1208 - val_acc: 0.9717\n",
      "Epoch 36/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1350 - acc: 0.9692 - val_loss: 0.1139 - val_acc: 0.9728\n",
      "Epoch 37/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1342 - acc: 0.9707 - val_loss: 0.1112 - val_acc: 0.9728\n",
      "Epoch 38/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1331 - acc: 0.9703 - val_loss: 0.1216 - val_acc: 0.9706\n",
      "Epoch 39/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1275 - acc: 0.9707 - val_loss: 0.1461 - val_acc: 0.9578\n",
      "Epoch 40/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1261 - acc: 0.9690 - val_loss: 0.1051 - val_acc: 0.9739\n",
      "Epoch 41/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1251 - acc: 0.9710 - val_loss: 0.1019 - val_acc: 0.9744\n",
      "Epoch 42/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1242 - acc: 0.9708 - val_loss: 0.1979 - val_acc: 0.9433\n",
      "Epoch 43/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1164 - acc: 0.9728 - val_loss: 0.1025 - val_acc: 0.9733\n",
      "Epoch 44/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1177 - acc: 0.9729 - val_loss: 0.1028 - val_acc: 0.9739\n",
      "Epoch 45/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.1139 - acc: 0.9725 - val_loss: 0.1024 - val_acc: 0.9711\n",
      "Epoch 46/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.1127 - acc: 0.9724 - val_loss: 0.0922 - val_acc: 0.9761\n",
      "Epoch 47/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.1102 - acc: 0.9740 - val_loss: 0.0941 - val_acc: 0.9756\n",
      "Epoch 48/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.1100 - acc: 0.9749 - val_loss: 0.0906 - val_acc: 0.9761\n",
      "Epoch 49/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1062 - acc: 0.9742 - val_loss: 0.0920 - val_acc: 0.9783\n",
      "Epoch 50/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1079 - acc: 0.9733 - val_loss: 0.0884 - val_acc: 0.9772\n",
      "Epoch 51/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1025 - acc: 0.9751 - val_loss: 0.0916 - val_acc: 0.9744\n",
      "Epoch 52/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1024 - acc: 0.9760 - val_loss: 0.0889 - val_acc: 0.9761\n",
      "Epoch 53/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1026 - acc: 0.9744 - val_loss: 0.0892 - val_acc: 0.9744\n",
      "Epoch 54/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1008 - acc: 0.9762 - val_loss: 0.0857 - val_acc: 0.9772\n",
      "Epoch 55/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.0999 - acc: 0.9754 - val_loss: 0.0922 - val_acc: 0.9772\n",
      "Epoch 56/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0971 - acc: 0.9771 - val_loss: 0.0823 - val_acc: 0.9794\n",
      "Epoch 57/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.0937 - acc: 0.9778 - val_loss: 0.0827 - val_acc: 0.9772\n",
      "Epoch 58/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0965 - acc: 0.9762 - val_loss: 0.0786 - val_acc: 0.9783\n",
      "Epoch 59/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.0926 - acc: 0.9785 - val_loss: 0.0760 - val_acc: 0.9767\n",
      "Epoch 60/70\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.0937 - acc: 0.9758 - val_loss: 0.0846 - val_acc: 0.9756\n",
      "Epoch 61/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.0883 - acc: 0.9797 - val_loss: 0.0854 - val_acc: 0.9739\n",
      "Epoch 62/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0884 - acc: 0.9783 - val_loss: 0.0729 - val_acc: 0.9778\n",
      "Epoch 63/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0861 - acc: 0.9793 - val_loss: 0.0785 - val_acc: 0.9767\n",
      "Epoch 64/70\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.0886 - acc: 0.9783 - val_loss: 0.0720 - val_acc: 0.9794\n",
      "Epoch 65/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0862 - acc: 0.9785 - val_loss: 0.1003 - val_acc: 0.9750\n",
      "Epoch 66/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0845 - acc: 0.9808 - val_loss: 0.0726 - val_acc: 0.9817\n",
      "Epoch 67/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0829 - acc: 0.9799 - val_loss: 0.0707 - val_acc: 0.9772\n",
      "Epoch 68/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0810 - acc: 0.9804 - val_loss: 0.0704 - val_acc: 0.9839\n",
      "Epoch 69/70\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.0807 - acc: 0.9817 - val_loss: 0.0796 - val_acc: 0.9744\n",
      "Epoch 70/70\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.0803 - acc: 0.9803 - val_loss: 0.0864 - val_acc: 0.9794\n",
      "Test loss: 0.0864\n",
      "Test accuracy: 0.979\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VOXZ8PHfNZN9gWyELUDYVxEQcV9Qq+CupYrVPmprba2t2rebdnF7a5e3tk/rU2urdanLoyJ1r4qiuKCghEUUZJVAwppA9mSSycz1/nEOYRKSMEAmM2Gu7+czn8w55z5nrkkm55pz3/e5b1FVjDHGGABPtAMwxhgTOywpGGOMaWFJwRhjTAtLCsYYY1pYUjDGGNPCkoIxxpgWlhRMXBGRx0Tk12GWLRaRsyIdkzGxxJKCMcaYFpYUjOmBRCQh2jGYI5MlBRNz3Gqbn4jIShGpE5GHRaSviLwuIjUiMl9EskPKXygiq0SkUkTeFZGxIdsmi8gyd79ngZQ2r3W+iKxw9/1IRCaGGeN5IrJcRKpFpERE7myz/WT3eJXu9mvc9aki8kcR2SwiVSKy0F13uoiUtvN7OMt9fqeIzBWRJ0WkGrhGRKaJyCL3NbaLyF9FJClk//Ei8paI7BGRnSLycxHpJyL1IpIbUu4YESkTkcRw3rs5sllSMLHqq8BXgFHABcDrwM+BPJzP7U0AIjIKeBq4BegDvAa8IiJJ7gnyReAJIAd4zj0u7r5TgEeA7wC5wD+Al0UkOYz46oD/ArKA84AbRORi97iD3Xj/x41pErDC3e9e4BjgRDemnwLBMH8nFwFz3dd8CggAP3R/JycAZwLfc2PIBOYDbwADgBHA26q6A3gXuCzkuFcBz6iqP8w4zBHMkoKJVf+jqjtVdSvwAfCxqi5X1UbgBWCyW+5y4D+q+pZ7UrsXSMU56R4PJAJ/VlW/qs4FloS8xreBf6jqx6oaUNV/AY3ufp1S1XdV9TNVDarqSpzEdJq7+Upgvqo+7b7ublVdISIe4JvAzaq61X3Nj9z3FI5Fqvqi+5oNqrpUVRerarOqFuMktb0xnA/sUNU/qqpPVWtU9WN3279wEgEi4gWuwEmcxlhSMDFrZ8jzhnaWM9znA4DNezeoahAoAQa627Zq61EfN4c8HwL8yK1+qRSRSmCQu1+nROQ4EVngVrtUAd/F+caOe4yN7eyWh1N91d62cJS0iWGUiLwqIjvcKqXfhBEDwEvAOBEZhnM1VqWqnxxiTOYIY0nB9HTbcE7uAIiI4JwQtwLbgYHuur0GhzwvAe5R1ayQR5qqPh3G6/4v8DIwSFV7A38H9r5OCTC8nX3KAV8H2+qAtJD34cWpegrVdkjjB4A1wEhV7YVTvXagGFBVHzAH54rmG9hVgglhScH0dHOA80TkTLeh9Ec4VUAfAYuAZuAmEUkQkUuBaSH7PgR81/3WLyKS7jYgZ4bxupnAHlX1icg04Osh254CzhKRy9zXzRWRSe5VzCPAn0RkgIh4ReQEtw1jHZDivn4i8EvgQG0bmUA1UCsiY4AbQra9CvQTkVtEJFlEMkXkuJDtjwPXABcCT4bxfk2csKRgejRVXYtTP/4/ON/ELwAuUNUmVW0CLsU5+VXgtD88H7JvEU67wl/d7RvcsuH4HnC3iNQAt+Mkp73H3QKci5Og9uA0Mh/tbv4x8BlO28Ye4PeAR1Wr3GP+E+cqpw5o1RupHT/GSUY1OAnu2ZAYanCqhi4AdgDrgekh2z/EaeBe5rZHGAOA2CQ7xsQnEXkH+F9V/We0YzGxw5KCMXFIRI4F3sJpE6mJdjwmdlj1kTFxRkT+hXMPwy2WEExbdqVgjDGmRcSuFETkERHZJSKfd7BdROQ+EdkgznAGUyIVizHGmPBEclCtx3B6dTzewfaZwEj3cRxOn+vjOijbIi8vTwsLC7smQmOMiRNLly4tV9W2977sJ2JJQVXfF5HCTopcBDzu3m26WESyRKS/qm7v7LiFhYUUFRV1YaTGGHPkE5HNBy4V3YbmgbS+bb/UXWeMMSZKopkUpJ117bZ6i8j1IlIkIkVlZWURDssYY+JXNJNCKc4YNXsV4Ixjsx9VfVBVp6rq1D59DlglZowx5hBFMym8DPyX2wvpeJyRGjttTzDGGBNZEWtoFpGngdOBPHdGqTtwxrZHVf+OMxnKuTjjzdQD10YqFmOMMeGJZO+jKw6wXYEbI/X6xhhjDp4Nc2GMMaaFJQVjjImmYBA+fRZ2rop2JEBk72g2xpi4FygpQn1VJIw4A6RNT/yanfDid2HjO2hyLypmPcempFFs3l1PXWMzg3PTGZaXzoCsVLye9nrxdz1LCsaYnqN2F2xZDIlpMGgapPTqksPWNTbz6sptVNb7GZqXzrA+6QzOSScpwUMgqJTXNrKjyseumkZSE730651M34xEMss/hcYaGD4dPF4AgkFl9fZqPtmwnYFL7+Wc6ucAWM9gXki/nHV5Z5GVkcqwioVcteP3JAd9PJZ0Nef5XiPzyUu4vekXrNKhreJLTwjy3YwPGDv9Ss467uj94u9KlhSMMeFpqISSj2HzR7BlETTWQu8C55E1CLKHwtBTIS3n0I4fDML25bBnU+v1TXVQusR53T0b9xXHQ3nGKDalTWRnciGDEqsZILvJ9u8gsW4H/rQ+bE0azkr/QN6pyGd9cADHjBzE6aP7cMLwXNKSEthYVssTizbz76Wl1DQ2txzbS4CzvMuYnrSGrf5MSoJ5bNNcdpDDBNnEmd7lnO5ZAeKMPF7MQB7zXMx/9GRqmz0MaN7CXxLvZ4KnmMW5F1OZM4nJWx7jp3V/YGvDk3zOCM4JfsAm7zD+0f/n1GQOpzH5Ir65/kZeTP5/7Lh4DkkFkyguqyHw2fOM/eI+sn2lbNyRw75J/CKjxw2dPXXqVLWxj0xcCgbB08XNgFs+hvf/AA0VkNLb+ead3Au8SdBYTaChCn9dJdSXk1y5EUFRTwIyYDKk5UFVKVSVgK/SOZ540EHHUTP4DFaln8CuWj/estWkVa4lp3Y96YEqajKGo33HkzHoaPKGjKN648cE1r5B/o736NW8u90w6zwZfCpjeM83kiXB0aRKI9M8a5gma5nsWU+K+AHYqVls0zzKJYdc3cNoKSFdGluOU6kZlGoeO8ijJmUAn9T1YYMMYei4qVx+0jhGpjdQt+gReq9+grSGHTRJMknauF88TYm9Kc45iZVpJ1DfFODM3U8x0LeeysS+rMmZztTyl/AkpuK5+H4Yc+6+v9/a/8D798L2FXD8jXDWHZAQMhX3nk3w2Hngb4Cv3A1LHoLtn0L+ePjKXTDirP2roMIkIktVdeoBy1lSMKYdlVvgs+eg5BMYdjpM+Cpk5He+TzAItTuheisE/K23peVC3sjw/6G3LoM3fwl15eCrgsZq50Rx4X0w5b/a3aWuwUflO39BarbjbarB01SFt6mGprS+1A05i8bC6aRm5pCa6CWpYh0ZC39D0obXCab3pT57FE21lajP2ccT9FOtqVRrGtWkUa1pfB4cyhIdzfLgCBJT0slNTyI3I5m8jCQGpjYzqHkL2dveZUz1R4yh9bf9ZjyUegqo8fRioH8zOdJ6bp9qTeUjmcTGrFPY02ssexqaqahroqLeT3UTeHKGMKZ/FmP6ZTK6XyaDctLITksiKy2RFAmgNdso02zW7/GzfmcNX5bXUZCdyrFDspiQVkli+WooX0+gsoSq7V/SXLGFXr5tpKhvXxDZhVC9DQJNMPQ0mPZtGDUTgv59ya9qK+SOcKqu3OoiAFRhw3znhF+y2Nn/kn9Ar/77/6FUwV8PSent/+13b3QSQ8126D0YzvgFHPW11q93CCwpmPgVDEJlsfOta+8/c2UJNOxpU1Ago4/zj5c1yKkG2fMlrJwDmz90ivQeDFVbQLxOvfFRlznJoarEOXZlifvcPWEE/W2jadGckkPjgGkEB50Aw06jIXssdU0B6puaaWgKkJWWxNC8dLwagL+fDPXlMORENLkX1ZpG4rpXqEsdyIcn/wuvR0jwCLvrmlhZWsmnJVUMKV/Ag4l/olZTqCaNGk2jllSGynZypBa/elkSHE0ZWZzvWUQdKfy9+QIeDcyggRQAkrweRvbNYFifDPIzk8nNSCIvI5mctCTq/QEq65uoqPNTUd/EnromymsbKatppLy2kbrGACPyM5gwsBfTcn1MbV5Gbq900gYdjTd/TMs34mAgyPZtWyjbsJSG7Wvw9B1H/6POYFCfXkg7SVNV211/2IJB52+7c9W+R2Y/mPpN6DP60I6p6nwxSM8/vKu6is1OddmES1tfSRwGSwomdvh9sG2Z8yEvX+d8Q0ru5VRVpPSG5N4hz3uBNxF81U6VRGO1s3/BsZA3Yv9jqzrH/PI92PkZ7FwNu1Y738T2Eg9kDoD0vNbf1IMB5x+4dmfrY+aNgomXoRNmUZE8kJ0bVuBd9Rz9Nr9Cr8Z9I7EE8VCXlEdtSn9qUvpRndSPyqS+VCT0ZVtdkK0VDeyo8hFQZYDsZpqsYZpnDUM8uwD4sf87zA2c1uqlkxM83NL7PW6oe4DnR/6eFxunsLK0ksp6Pz9OeJbvel9hSuPfqSajZZ/stESOHpTFjxvuY3TFuyy9rAhvYhJej+AVobm5mYTtS8ncMp/crQtIr9vCusGX8+mQb1KX0JugKv16pzK2XyaFeekkeq2n+pHIkoLpGqpOI98Xrzgn0b11zim9nG9VfSdARt/WJ1tflVPtsrdBcutS55IcoFcBNDc4J/1OvlW3K3cEjJrhPILNsG4erHsdKoqd7ak50He8E1PfcZAzHH/mQOqT8qkPCnWNARqaAtS538zrmppp9AdpbvKRULuV5Lqt7PSns6RhIFsqGtiyp57akMZHIcjp6SWkeZspbs6luKkXdc37n0ATvUJBdhoj8jMYmZ/ByL4Z5KQn0+BeFWj1dk5YcSv5NZ/zzmlzCeaMJDXJS1lNI8UlJXxn5df4Qgu5ovE2RvXtxaRBvZlYkMW0hI0Mf/lidp19P9UjLqI5qKQnJVCQnYpoEO4dCcPPgK/+88B/00h88zYxLdykYL2PTPvKN8Bnc2Dls85J15sEnkTw1+1fdu/JOGsI7PjUuQzXIHgSoP8kOO47MPhEGHz8vp4pqtDscxKIr9q5IvBVob4q/H4/Pk86Dd506iUdXwAyti2id8l8Mj5+EM+ivwLQLEl8kTKZeYkzeKFmLNsa80io9uDZ4FStNAWq8AcqgXBvCupNktdDQU4tQ3LSOLYwm0E5aQzJTWdIbhqDstNITWpdrxsIKk3NwZbqHE9YfckLYMoT8MBJnL36Nrju7X1VBDvvA61j2nf/wdq8sSSEfmsPDoT5eeRvf5f8E69qfcgtH0P9bhh97oFf3hKC6YQlBeMIBpxv9OvegLVvwK5VgMCw0+DUn8LYC5yrg0CzcwJvrHbq03et3lcfu+4N5xv6qT+FISc4VT5uY1owqCzcUM7Hm9bQJyOZgdlpDMhKoV+vbDY3JLNscwLLtgjLNsOO6r2Nf/XuA2AEMII0ruEkz+cEERbrePpm5DK2oBcX56XhESEQVAJBpTmoJCV4SEv0kpacQFqS130kkJ7kJdV9npLoITnBS1KCh+QED6mJ3jBP7A6vR/ZLFGHpNQAu/hs8PRvm3wkzfgu7voAl/4RjroW+4/f/5/R4YdQ5sOY/zt/BG1JizX+cpD3irIOPxZgQlhTi3fZPYcnDzkmlvtxpUB1yIpx9j9PI1WtA6/LeBOfbfloOwd5D+DJ9MisTK/lUK/nSU8eo3EyO7ZPDsfnZ5CYlU9XgZ+7SUp5cvJlN5XWIOBcJ7SnITuW4YTmM6ptJRsuJPIHUJE+bhsZTyE1P4q/5mYd2Qo4Vo2fCcd+FxX9zejgtfgCSM2D6LzreZ9Q5sOIp536BwpOcdarO32/YaV12M5eJX5YUjhQNlbDxbdi4wLnr062OwVftnCgGn+Cc7Aef4DS4rn4JPnkISj9x7g4dc55TVz/iTEjNbnXoGp+f99aVsXl3PTurfeyo8rGz2seXZXUtN/ykJXkpzE3niU2beXih0x1xWJ90tlf6aPAHmDI4i5svn8SMCf2obWxma0UD2yob2FblY2BWClMGZ5PfK6Xbf21Rd9ZdUPwhPHeN0zg+4/eQnttx+eFnOFcE617flxTK1kDFJjjxB90SsjmyWVLoySq3wOqXnWqbLYucxtfUbOg9yOnJkzPMaRSu3eG0DRQ97OyXkOLU5+eOoOms3/BW0hnUSQaD0tIY3JhCv2SnnnzB2l288uk23lmzi8bmIAC9UhLo1zuFvr1SuHDSAI4elMXRBVmMyM/A6xEamwN8VlrFJ8V7WFpcwbTCHK46fggTBvZuCTsl0UteRjJHD8qKxm8ttiSmwKxH4MHTIG80HPutzssnZ0LhyU4j+9m/dtat+Y/zM5z2BGMOwJJCT9NQAatedPrSb/nIWZc/Hk68yamOGHhM+ze5BJqdLpubF6G7N/Bl3nQe3jqYl9/aQW3jl62KJnk9eDzg8wfJy0jmimmDOX9if8YP6H3A6prkBC9TC3OYWniIQx3Eoz6j4NvvQEqW0x33QEbPhNd/6tzklDsc1r7m/N3bu1HKmINkSSFWNFQ6A2tlDdp/m98H6+c5iWD9m073zrzRcMav4KhZzp2YQFNzkI/Wl9MrNZER+Rn0Stl3gqkPwJLaAhZVnsG7GyawZmENKYnbOO+oAVx+7CD6905h8+56tuxxHj5/gLPH9eW4YbndNjpjXMsfG37ZUec4SWHdPBh/idNB4IxfRS42E1csKcSCVS/CKzc7N2v1KnB67gw+wakGWvMKrHoJGquc+wGmXQ8TL4N+E1u6FlbV+3nqk83866NidlbvG6elX68URvbNoKEpwIqSSpqDSoJHmDw4i3sumcAFRw9olTgG5aR1+1s3hyC7EPqMddoV9nZlHXNeVEMyRw5LCtHUWAtv3ArLn4ABU5yTfcnHsOkDZ9wdgKQMGHuhs23oqa2qhjaW1fL4R8XMKSqlwR/glJF5/PrioxBg/a5a1u+qYcOuWjwiXHfKME4cnsvUwmzSkuzP3uONOgcW/RWaG522oz5joh2ROULY2SFati2Hud9yxto5+f/A9J879cnH3+B0MazYRFPZRhKHnoiEDJxV39TMf1ZuZ05RCUuKK0j0ChcePZDrThnK2P77uiOeNa5vNN6V6S6jZ8KHf3a+RJzwfbshzXQZSwrdrbkJFv7JGa44PR+ufgWGntKyORBU3lmzi0c/3MVHG5tITnifPHckyt5pSSzbXEFtYzPD8tK5deYYLp0ykPzMOOzKGe8KjnXuJG/YY1VHpktZUuhO25bDizc6dwsf9TWY+f9ahn2o8fmZU1TKvz4qZsueevr3TuGG04c7sz7VNFJW20h5TSNnj+/L7GMHc2xhdmRGjjQ9g8cLY8+H9W/BoOOiHY05glhS6A5+H7z3O/jwPkjvA7Ofbpl4Q1V5YflWfvPaF5TXNjF1SDY/mzGGc8b3bT3ujTFtzfi90+voMMfZNyaUJYXu8MpNzs1jk69yho9IdW7aWr+zhl+++Dkfb9rD5MFZ/PPqY5lkN3SZcCWlOQ9jupAlhUjbshhWPkv9cbewZfKPKC9torx2KytLq3h8UTHpyQn89tKjuHzqoIMaiM0YYyLBkkIkBYMEXvspVd48TnpvIg3vfdBq86xjCrht5hhyM7pmZiVjjDlclhQiqOrjx+m941P+r/97fGv6BMb0z6RPRjJ5mcnkZyaTmRLGkAbGGNONLClEyLotW8mddwcrdCQXXnUL08fafQPGmNhn3Vsi4IP1Zbz/8G3kUknmxX+0hGCM6THsSqELqSqPfljMk68t4I2k/1A/9jKGTz7twDsaY0yMsKTQRXz+AL944XP+vayU53PmkuhPJmnm3dEOyxhjDoolhS6wo8rHd55cyqcllfzypAymLP0QTrvVxrc3xvQ4lhQOU1HxHr775DIampr5xzeO4Zyquc6Goy+PbmDGGHMIrKH5EKkqjy8qZvaDi8lI9vLCjSdxzvh+ztzH/SY6wxkbY0wPY1cKhyC0/eDMMfn86fJJ9E5NhKqtUPqJzYJljOmxLCkcpNKKer775FI+31rNzWeO5OYzR+4bnuKLV5yf4y6OXoDGGHMYLCkchPqmZr7+0MdU1Dfx8NVTObPt/QerX4L88ZA3IjoBGmPMYbI2hYPwxzfXsWVPPQ9ffez+CaFmB2xZBOMuik5wxhjTBSwphGlFSSWPfriJq44fzLShOfsX+OIVQC0pGGN6tIgmBRGZISJrRWSDiNzazvYhIvK2iKwUkXdFpCCS8RyqpuYgP5u7kvzMFH42o4MJ0le/BHmjId8mUDfG9FwRSwoi4gXuB2YC44ArRGRcm2L3Ao+r6kTgbuC3kYrncPzjvY2s3VnDry+e0P7IprVlsPlDu0owxvR4kbxSmAZsUNUvVbUJeAZoe9YcB7ztPl/Qzvao27Crhv95ZwPnT+zPWeM6GNhuzaugQUsKxpgeL5JJYSBQErJc6q4L9SnwVff5JUCmiOS2PZCIXC8iRSJSVFZWFpFg2xMMKrf++zPSkr3ceeH4jguufglyhkPfTsoYY0wPEMmk0N7cktpm+cfAaSKyHDgN2Ao077eT6oOqOlVVp/bp06frI+3Ae+vLKNpcwc/PHUteR7Oj1e+BTe87Vwli02kaY3q2SN6nUAoMClkuALaFFlDVbcClACKSAXxVVasiGNNBmff5DjKSE7ho0oCOC617AzQA4y7svsCMMSZCInmlsAQYKSJDRSQJmA28HFpARPJEZG8MtwGPRDCegxIIKm+t3sn0MfkkJ3g7LrjpfUjLg/6Tui84Y4yJkIglBVVtBr4PzAO+AOao6ioRuVtE9n6tPh1YKyLrgL7APZGK52At3VzB7romzhnfyaxpqlC8EApPtqojY8wRIaLDXKjqa8BrbdbdHvJ8LjA3kjEcqnmrdpCU4OH00fkdF6rcDFUlcNLN3ReYMcZEkN3R3A5VZd6qHZw8Io+M5E7yZvFC52fhKd0TmDHGRJglhXas2lZNaUUDM8b367xg8UKnPaHP6O4JzBhjIsySQjveXLUDj8CZYzupOrL2BGPMEciSQjvmrdrJsYU55HZ0bwLsa08oPLn7AjPGmAizpNBGcXkda3fWOFNrdlrQ2hOMMUceSwptzFu1A4CzO+uKCtaeYIw5IllSaOONVTs4amBvCrLTOi7U0p5wkrUnGGOOKJYUQuys9rF8S2XnN6xBSHuCVR0ZY44slhRCvLl6J8BBtCdYI7Mx5shiSSHEoo3lDMpJZUR+RucFixdCWi70sVnWjDFHFksKIb4sq2NUfibSWTuB3Z9gjDmCWVJwBYNK8e46huald17Q2hOMMUcwSwquHdU+fP4gQ/scIClYe4Ix5ghmScG1qbwO4MBXCtaeYIw5gllScH3pJoVheZ00Mjc3wfq3YOhp1p5gjDkiWVJwFZfXkZropW+vTsY7WvcG1JfD0Vd0X2DGGNONLCm4NpXXUZiX3nnPo2WPQ+YAGHFm9wVmjDHdyJKCa1N5HcM6a0+oKoUN82HyleDpZM5mY4zpwSwpAP5AkC176jtvZF7xv4DC5Ku6LS5jjOlulhSAkj31BILacVIIBmHZEzDsdMgu7MbIjDGme1lSIKQ7akf3KGx6F6q2wORvdF9QxhgTBZYUCEkKuR0khWVPQGo2jDm/G6MyxpjuZ0kBJylkpSWSnZ60/8a63bDmVZg4GxJTuj84Y4zpRpYUcJJCh+0JK5+FQBNMsaojY8yRz5ICnSQFVVj+BAw8BvqO7/7AjDGmm8V9UqhvamZ7la/9exS2r4Bdq62B2RgTN+I+KRSX1wNQ2F5SWPs6IDD2wu4NyhhjoiTuk0Kno6OumweDpkF6bjdHZYwx0WFJobwWgMK23VFrdjjVRyPPjkJUxhgTHZYUyuvp1yuF9OSE1hvWv+X8HHVO9wdljDFRYkmhvLb9qqP186DXQOg7ofuDMsaYKLGk4A6Z3UpzI2xcACO/YpPpGGPiSlwnhYq6Jirq/ft3R938ETTVwkirOjLGxJewkoKI/FtEzhORIyqJbNrdQc+j9W+CNxmGnRaFqIwxJnrCPck/AHwdWC8ivxORI2LW+k1lHYyOum4eDD0FkjqZX8EYY45AYSUFVZ2vqlcCU4Bi4C0R+UhErhWRxEgGGEmbyuvweoRB2Wn7Vu7eCHs2WtWRMSYuhV0dJCK5wDXAdcBy4C84SeKtiETWDTbtrmNQdipJCSG/hnXznJ+j7P4EY0z8SThwERCR54ExwBPABaq63d30rIgURSq4SNtU1k7Po3VvQN5om2HNGBOXwr1S+KuqjlPV34YkBABUdWpHO4nIDBFZKyIbROTWdrYPFpEFIrJcRFaKyLkHGf8hU9X9R0dtrHF6HtlVgjEmToWbFMaKSNbeBRHJFpHvdbaDiHiB+4GZwDjgChEZ16bYL4E5qjoZmA38LezID1O1r5kGf4ABvVP3rdy4AIJ+GDWju8IwxpiYEm5S+LaqVu5dUNUK4NsH2GcasEFVv1TVJuAZ4KI2ZRTo5T7vDWwLM57D5vMHAEhL9u5bufY1SO4Ng47rrjCMMSamhJsUPCL7bu11rwLambuylYFASchyqbsu1J3AVSJSCrwG/KC9A4nI9SJSJCJFZWVlYYbcuYYmJymkJrpJoaECVr0IEy4Fb4/tUGWMMYcl3KQwD5gjImeKyBnA08AbB9invfEhtM3yFcBjqloAnAs80d4Ncqr6oKpOVdWpffr0CTPkzjX42ySFT5+B5gaY+s0uOb4xxvREYfU+An4GfAe4Aedk/ybwzwPsUwoMClkuYP/qoW8BMwBUdZGIpAB5wK4w4zpke5NCSpLXmXaz6BEoOBb6T4z0SxtjTMwKKymoahDnruYHDuLYS4CRIjIU2IrTkPz1NmW2AGcCj4nIWCAF6Jr6oQPwhVYfFS+E8nVw8cG8PWOMOfKEe5/CSOC3OL2IUvauV9VhHe2jqs0i8n2cqicv8IiqrhKRu4EiVX0Z+BHwkIj8EKdq6RpVbVvFFBGtqo8WPwwpWTD+ku54aWOMiVnhVh89CtwB/DcwHbiW9tsMWlHV13AakEPX3R434/gDAAAVzElEQVTyfDVwUrjBdqW9SSGjeQ988QpM+w4kph5gL2OMObKF29CcqqpvA6Kqm1X1TuCMyIUVeXt7H+WuexaCzTD12ihHZIwx0RfulYLP7RW03q0S2grkRy6syPP5A3gIkrnqKRh6KuSNjHZIxhgTdeFeKdwCpAE3AccAVwFXRyqo7tDgD3Ca51O81SUw9VvRDscYY2LCAa8U3BvVLlPVnwC1OO0JPV5DU5ArvfPRjL7ImPOiHY4xxsSEA14pqGoAOCb0juYjgd9Xx3TPCuTo2XYHszHGuMJtU1gOvCQizwF1e1eq6vMRiaobeHwVeEUhZ3i0QzHGmJgRblLIAXbTuseRAj02KeCrcn6m9Oq8nDHGxJFw72g+ItoRQnka9yaF3tENxBhjYki4dzQ/yv6D2aGqPXb0OG9TtfPEkoIxxrQIt/ro1ZDnKcAldOPcB5GQ4N+bFLI6L2iMMXEk3Oqjf4cui8jTwPyIRNRNEv01zhO7UjDGmBbh3rzW1khgcFcG0t2SmmudJ8nW0GyMMXuF26ZQQ+s2hR04cyz0WMnNNTRKCskJB5pAzhhj4ke41UeZkQ6ku6UGavF5M0iOdiDGGBNDwqo+EpFLRKR3yHKWiFwcubAiLzVYS2NCRrTDMMaYmBJum8Idqlq1d0FVK3HmV+ix0rSOpoQj7gLIGGMOS7hJob1y4XZnjTnBoJKhdfgTLSkYY0yocJNCkYj8SUSGi8gwEflvYGkkA4ukxuYgvaijOdF6HhljTKhwk8IPgCbgWWAO0ADcGKmgIq3BHyBTGggkWVIwxphQ4fY+qgNujXAs3aahqZk+1FFm9ygYY0wr4fY+ektEskKWs0VkXuTCiixffR1JEkDtbmZjjGkl3OqjPLfHEQCqWkEPnqPZX7fHeWJJwRhjWgk3KQRFpGVYCxEppJ1RU3sKf72T3zyWFIwxppVwu5X+AlgoIu+5y6cC10cmpMgL1DlJwZtuScEYY0KF29D8hohMxUkEK4CXcHog9UiBBjcppGVHORJjjIkt4Q6Idx1wM1CAkxSOBxbRenrOHkPdpJCYZnMpGGNMqHDbFG4GjgU2q+p0YDJQFrGoIs3nTLCTlJET5UCMMSa2hJsUfKrqAxCRZFVdA4yOXFgR5s7PnJxhVwrGGBMq3IbmUvc+hReBt0Skgh48HaensRqfJpKSmh7tUIwxJqaE29B8ifv0ThFZAPQG3ohYVBGW0FRNDWnkJRzqxHPGGHNkOuiRTlX1vQOXim1OUkinj0i0QzHGmJgSl1+VE5trqBOrOjLGmLbiMikk+Wuo89isa8YY01ZcJoXkQC0NlhSMMWY/cZkUUgO1+LyWFIwxpq34TArBWhptfmZjjNlP/CUFv48k/DQl2JWCMca0FX9JwefczexPtCsFY4xpK26Tgs3PbIwx+4toUhCRGSKyVkQ2iMh+czyLyH+LyAr3sU5EKts7TpeypGCMMR066DuawyUiXuB+4CtAKbBERF5W1dV7y6jqD0PK/wBn9NXIcpNCMNmSgjHGtBXJK4VpwAZV/VJVm4BngIs6KX8F8HQE43G4I6SqTcVpjDH7iWRSGAiUhCyXuuv2IyJDgKHAOx1sv15EikSkqKzs8KZxCLjzM0uKDZttjDFtRTIptDfanHZQdjYwV1UD7W1U1QdVdaqqTu3Tp89hBeV3k4In1a4UjDGmrUgmhVJgUMhyAR3PwTCb7qg6AgJ1FTSpl8TktO54OWOM6VEimRSWACNFZKiIJOGc+F9uW0hERgPZOHM+R1ywoYpq0klNilgbuzHG9FgRSwqq2gx8H5gHfAHMUdVVInK3iFwYUvQK4BlV7ahqqWvj8lVSrWmkJnm74+WMMaZHiejXZVV9DXitzbrb2yzfGckY9tNQRTVppCZaUjDGmLbi747mpmqqNZ0USwrGGLOfuEsK3sZq50rBqo+MMWY/8ZcUmqqdNgW7UjDGmP3EXVJIaKqhxtoUjDGmXfGVFJobSQj6nDaFpPh668YYE474OjP6qgGs95ExxnQgzpKCMxhetaZZ7yNjjGlHXCaFek86id74euvGGBOO+DozusNm+xJsKk5jjGlPfCWFvfMzW1Iwxph2xWdSSLRZ14wxpj1xmRQCSXalYIwx7Ym7pBDAgySlRzsSY4yJSXGXFOokw+ZSMMaYDsRdUqiVdLtxzRhjOhBnSaGaGtJIsRFSjTGmXXGWFNypOO1KwRhj2hV3SaEqaOMeGWNMR+IwKaTaBDvGGNOBuEoK6qtiTzDVBsMzxpgOxE9SCPgRf53NumaMMZ2In6TQMpdCOqmJ8fO2jTHmYMTP2dFXCThzKVibgjHGtC9+kkLjvlnXrE3BGGPaFz9JoWXWNbtPwRhjOhJ/SQGrPjLGmI7EXVKosd5HxhjTobhLCtamYIwxHYufpDD0NFYe9XPqSLHqI2OM6UD8TCzQfyJfDM5Cl3xm1UfGxBC/309paSk+ny/aoRwRUlJSKCgoIDEx8ZD2j5+kADQ0BQAsKRgTQ0pLS8nMzKSwsBARiXY4PZqqsnv3bkpLSxk6dOghHSN+qo+ABn8QwKqPjIkhPp+P3NxcSwhdQETIzc09rKuuOEsKzpVCckJcvW1jYp4lhK5zuL/LuDo7+vwBUhO99gE0xpgOxFVSaGgKWNWRMaaVyspK/va3vx30fueeey6VlZWdlrn99tuZP3/+oYYWFfGVFNwrBWOM2aujpBAIBDrd77XXXiMrK6vTMnfffTdnnXXWYcXX3eKr95E/QIoNm21MzLrrlVWs3lbdpcccN6AXd1wwvsPtt956Kxs3bmTSpEkkJiaSkZFB//79WbFiBatXr+biiy+mpKQEn8/HzTffzPXXXw9AYWEhRUVF1NbWMnPmTE4++WQ++ugjBg4cyEsvvURqairXXHMN559/PrNmzaKwsJCrr76aV155Bb/fz3PPPceYMWMoKyvj61//Ort37+bYY4/ljTfeYOnSpeTl5XXp7yFccXWG9Fn1kTGmjd/97ncMHz6cFStW8Ic//IFPPvmEe+65h9WrVwPwyCOPsHTpUoqKirjvvvvYvXv3fsdYv349N954I6tWrSIrK4t///vf7b5WXl4ey5Yt44YbbuDee+8F4K677uKMM85g2bJlXHLJJWzZsiVybzYMEb1SEJEZwF8AL/BPVf1dO2UuA+4EFPhUVb8eqXis+siY2NbZN/ruMm3atFZ9/O+77z5eeOEFAEpKSli/fj25ubmt9hk6dCiTJk0C4JhjjqG4uLjdY1966aUtZZ5//nkAFi5c2HL8GTNmkJ2d3aXv52BFLCmIiBe4H/gKUAosEZGXVXV1SJmRwG3ASapaISL5kYoHnKSQkRxXNWbGmIOUnp7e8vzdd99l/vz5LFq0iLS0NE4//fR27wFITk5uee71emloaGj32HvLeb1empubAeeGs1gSyeqjacAGVf1SVZuAZ4CL2pT5NnC/qlYAqOquCMbj9D6yKwVjTIjMzExqamra3VZVVUV2djZpaWmsWbOGxYsXd/nrn3zyycyZMweAN998k4qKii5/jYMRyaQwECgJWS5114UaBYwSkQ9FZLFb3bQfEbleRIpEpKisrOyQA/L5rU3BGNNabm4uJ510EhMmTOAnP/lJq20zZsygubmZiRMn8qtf/Yrjjz++y1//jjvu4M0332TKlCm8/vrr9O/fn8zMzC5/nXBJpC5dRORrwDmqep27/A1gmqr+IKTMq4AfuAwoAD4AJqhqh51/p06dqkVFRYcU03G/mc/00fn87qsTD2l/Y0zX++KLLxg7dmy0w4iaxsZGvF4vCQkJLFq0iBtuuIEVK1Yc1jHb+52KyFJVnXqgfSNZwV4KDApZLgC2tVNmsar6gU0ishYYCSyJREANTQGbS8EYE1O2bNnCZZddRjAYJCkpiYceeiiq8UQyKSwBRorIUGArMBto27PoReAK4DERycOpTvoyUgH5/EGrPjLGxJSRI0eyfPnyaIfRImJtCqraDHwfmAd8AcxR1VUicreIXOgWmwfsFpHVwALgJ6q6fyfgLtAcCNIUCFpDszHGdCKi/TNV9TXgtTbrbg95rsD/cR8R5Wt2h822pGCMMR2Kmzua906wk2LVR8YY06G4SQo+v826ZowxBxI3SaHBkoIxpgtkZGQAsG3bNmbNmtVumdNPP50DdZ3/85//TH19fctyOENxd4f4SQp752dOipu3bIyJoAEDBjB37txD3r9tUghnKO7uEDcDAe29UrD7FIyJYa/fCjs+69pj9jsKZu43FmeLn/3sZwwZMoTvfe97ANx5552ICO+//z4VFRX4/X5+/etfc9FFrUfpKS4u5vzzz+fzzz+noaGBa6+9ltWrVzN27NhWYx/dcMMNLFmyhIaGBmbNmsVdd93Ffffdx7Zt25g+fTp5eXksWLCgZSjuvLw8/vSnP/HII48AcN1113HLLbdQXFzc4RDdXSluvjZb9ZExpj2zZ8/m2WefbVmeM2cO1157LS+88ALLli1jwYIF/OhHP+p04LoHHniAtLQ0Vq5cyS9+8QuWLl3asu2ee+6hqKiIlStX8t5777Fy5UpuuukmBgwYwIIFC1iwYEGrYy1dupRHH32Ujz/+mMWLF/PQQw+13McQ7hDdhyNurhR8LdVHlhSMiVmdfKOPlMmTJ7Nr1y62bdtGWVkZ2dnZ9O/fnx/+8Ie8//77eDwetm7dys6dO+nXr1+7x3j//fe56aabAJg4cSITJ+4bSmfOnDk8+OCDNDc3s337dlavXt1qe1sLFy7kkksuaRmt9dJLL+WDDz7gwgsvDHuI7sMRN0nBrhSMMR2ZNWsWc+fOZceOHcyePZunnnqKsrIyli5dSmJiIoWFhe0OmR1KRPZbt2nTJu69916WLFlCdnY211xzzQGP09kVSbhDdB8Oqz4yxsS92bNn88wzzzB37lxmzZpFVVUV+fn5JCYmsmDBAjZv3tzp/qeeeipPPfUUAJ9//jkrV64EoLq6mvT0dHr37s3OnTt5/fXXW/bpaMjuU089lRdffJH6+nrq6up44YUXOOWUU7rw3XYufq4UrPrIGNOB8ePHU1NTw8CBA+nfvz9XXnklF1xwAVOnTmXSpEmMGTOm0/1vuOEGrr32WiZOnMikSZOYNm0aAEcffTSTJ09m/PjxDBs2jJNOOqlln+uvv56ZM2fSv3//Vu0KU6ZM4Zprrmk5xnXXXcfkyZMjUlXUnogNnR0phzp09purdvDC8q3cd8VkEr1xc4FkTMyL96GzIyFWh86OKWeP78fZ49tvJDLGGOOwr8zGGGNaWFIwxkRdT6vGjmWH+7u0pGCMiaqUlBR2795tiaELqCq7d+8mJSXlkI8RN20KxpjYVFBQQGlpKWVlZdEO5YiQkpJCQUHBIe9vScEYE1WJiYkMHTo02mEYl1UfGWOMaWFJwRhjTAtLCsYYY1r0uDuaRaQM6Hwgko7lAeVdGE6k9bR4oefFbPFGlsUbWQcT7xBV7XOgQj0uKRwOESkK5zbvWNHT4oWeF7PFG1kWb2RFIl6rPjLGGNPCkoIxxpgW8ZYUHox2AAepp8ULPS9mizeyLN7I6vJ446pNwRhjTOfi7UrBGGNMJywpGGOMaRE3SUFEZojIWhHZICK3RjuetkTkERHZJSKfh6zLEZG3RGS9+zM7mjGGEpFBIrJARL4QkVUicrO7PiZjFpEUEflERD51473LXT9URD52431WRJKiHWsoEfGKyHIRedVdjtl4RaRYRD4TkRUiUuSui8nPA4CIZInIXBFZ436OT4jxeEe7v9u9j2oRuaWrY46LpCAiXuB+YCYwDrhCRMZFN6r9PAbMaLPuVuBtVR0JvO0ux4pm4EeqOhY4HrjR/Z3GasyNwBmqejQwCZghIscDvwf+2423AvhWFGNsz83AFyHLsR7vdFWdFNJ3PlY/DwB/Ad5Q1THA0Ti/55iNV1XXur/bScAxQD3wAl0ds6oe8Q/gBGBeyPJtwG3RjqudOAuBz0OW1wL93ef9gbXRjrGT2F8CvtITYgbSgGXAcTh3gya09zmJ9gMocP/JzwBeBSTG4y0G8tqsi8nPA9AL2ITb2SbW420n/rOBDyMRc1xcKQADgZKQ5VJ3Xazrq6rbAdyf+VGOp10iUghMBj4mhmN2q2JWALuAt4CNQKWqNrtFYu1z8Wfgp0DQXc4ltuNV4E0RWSoi17vrYvXzMAwoAx51q+f+KSLpxG68bc0Gnnafd2nM8ZIUpJ111he3C4hIBvBv4BZVrY52PJ1R1YA6l94FwDRgbHvFujeq9onI+cAuVV0aurqdojERr+skVZ2CU017o4icGu2AOpEATAEeUNXJQB0xVFXUGbcd6ULguUgcP16SQikwKGS5ANgWpVgOxk4R6Q/g/twV5XhaEZFEnITwlKo+766O6ZgBVLUSeBenLSRLRPZONhVLn4uTgAtFpBh4BqcK6c/Ebryo6jb35y6cuu5pxO7noRQoVdWP3eW5OEkiVuMNNRNYpqo73eUujTleksISYKTbcyMJ59Lr5SjHFI6Xgavd51fj1NvHBBER4GHgC1X9U8immIxZRPqISJb7PBU4C6dhcQEwyy0WM/Gq6m2qWqCqhTif13dU9UpiNF4RSReRzL3Pceq8PydGPw+qugMoEZHR7qozgdXEaLxtXMG+qiPo6pij3WDSjQ0z5wLrcOqRfxHteNqJ72lgO+DH+RbzLZw65LeB9e7PnGjHGRLvyThVFyuBFe7j3FiNGZgILHfj/Ry43V0/DPgE2IBzOZ4c7Vjbif104NVYjteN61P3sWrv/1isfh7c2CYBRe5n4kUgO5bjdWNOA3YDvUPWdWnMNsyFMcaYFvFSfWSMMSYMlhSMMca0sKRgjDGmhSUFY4wxLSwpGGOMaWFJwZhuJCKn7x3x1JhYZEnBGGNMC0sKxrRDRK5y519YISL/cAfTqxWRP4rIMhF5W0T6uGUnichiEVkpIi/sHc9eREaIyHx3DodlIjLcPXxGyDj+T7l3hxsTEywpGNOGiIwFLscZ4G0SEACuBNJxxpyZArwH3OHu8jjwM1WdCHwWsv4p4H515nA4EeeOdXBGlL0FZ26PYTjjHBkTExIOXMSYuHMmziQmS9wv8ak4g4wFgWfdMk8Cz4tIbyBLVd9z1/8LeM4dB2igqr4AoKo+APd4n6hqqbu8AmcejYWRf1vGHJglBWP2J8C/VPW2VitFftWmXGdjxHRWJdQY8jyA/R+aGGLVR8bs721glojkQ8s8w0Nw/l/2jlD6dWChqlYBFSJyirv+G8B76swtUSoiF7vHSBaRtG59F8YcAvuGYkwbqrpaRH6JM4uYB2fk2htxJmIZLyJLgSqcdgdwhiv+u3vS/xK41l3/DeAfInK3e4yvdePbMOaQ2CipxoRJRGpVNSPacRgTSVZ9ZIwxpoVdKRhjjGlhVwrGGGNaWFIwxhjTwpKCMcaYFpYUjDHGtLCkYIwxpsX/B6TFO5r3lf5DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for layers in range(1, 5):\n",
    "model = create_dense([32] * 5)\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test, y_test))\n",
    "def evaluate(model, batch_size=128, epochs=5):\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['training', 'validation'], loc='best')\n",
    "# plt.show()\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in range(1, 5):\n",
    "    model = create_dense([32] * layers)\n",
    "    evaluate(model)\n",
    "\n",
    "# model.save('model/model_9_positions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
