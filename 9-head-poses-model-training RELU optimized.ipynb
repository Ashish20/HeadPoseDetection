{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Head poses Model Training\n",
    "\n",
    "Neural Network model for a quick Head Pose estimation, using just facial points as input. The model was made for real-time control of a Pan-Tilt camera using face movements as the control signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies, set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(14)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Dropout, Activation, Conv2D, MaxPooling2D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import regularizers \n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 61)\n"
     ]
    }
   ],
   "source": [
    "dfCenter = pd.read_csv('datasets/face_center.csv')\n",
    "dfCenter2 = pd.read_csv('datasets/face_center2.csv')\n",
    "dfCenter = dfCenter.append(dfCenter2, ignore_index = True)\n",
    "\n",
    "dfRight = pd.read_csv('datasets/face_right.csv')\n",
    "dfRight2 = pd.read_csv('datasets/face_right2.csv')\n",
    "dfRight = dfRight.append(dfRight2, ignore_index = True)\n",
    "\n",
    "dfLeft = pd.read_csv('datasets/face_left.csv')\n",
    "dfLeft2 = pd.read_csv('datasets/face_left2.csv')\n",
    "dfLeft = dfLeft.append(dfLeft2, ignore_index = True)\n",
    "\n",
    "dfUp = pd.read_csv('datasets/face_up.csv')\n",
    "dfUp2 = pd.read_csv('datasets/face_up2.csv')\n",
    "dfUp = dfUp.append(dfUp2, ignore_index = True)\n",
    "\n",
    "dfDown = pd.read_csv('datasets/face_down.csv')\n",
    "dfDown2 = pd.read_csv('datasets/face_down2.csv')\n",
    "dfDown = dfDown.append(dfDown2, ignore_index = True)\n",
    "\n",
    "dfUpRight = pd.read_csv('datasets/face_up_right.csv')\n",
    "dfUpRight2 = pd.read_csv('datasets/face_up_right2.csv')\n",
    "dfUpRight = dfUpRight.append(dfUpRight2, ignore_index = True)\n",
    "\n",
    "dfUpLeft = pd.read_csv('datasets/face_up_left.csv')\n",
    "dfUpLeft2 = pd.read_csv('datasets/face_up_left2.csv')\n",
    "dfUpLeft = dfUpLeft.append(dfUpLeft2, ignore_index = True)\n",
    "\n",
    "dfDownRight = pd.read_csv('datasets/face_down_right.csv')\n",
    "dfDownRight2 = pd.read_csv('datasets/face_down_right2.csv')\n",
    "dfDownRight = dfDownRight.append(dfDownRight2, ignore_index = True)\n",
    "\n",
    "dfDownLeft = pd.read_csv('datasets/face_down_left.csv')\n",
    "dfDownLeft2 = pd.read_csv('datasets/face_down_left2.csv')\n",
    "dfDownLeft = dfDownLeft.append(dfDownLeft2, ignore_index = True)\n",
    "\n",
    "columns = list(dfCenter)\n",
    "\n",
    "dfCenter['RESULT'] = 0\n",
    "dfRight['RESULT'] = 1\n",
    "dfLeft['RESULT'] = 2\n",
    "dfUp['RESULT'] = 3\n",
    "dfDown['RESULT'] = 4\n",
    "dfUpRight['RESULT'] = 5\n",
    "dfUpLeft['RESULT'] = 6\n",
    "dfDownRight['RESULT'] = 7\n",
    "dfDownLeft['RESULT'] = 8\n",
    "\n",
    "\n",
    "df = dfCenter.append(dfRight, ignore_index=True).append(dfLeft, ignore_index = True)\n",
    "df = df.append(dfUp, ignore_index = True).append(dfDown, ignore_index = True)\n",
    "df = df.append(dfUpRight, ignore_index = True).append(dfUpLeft, ignore_index = True)\n",
    "df = df.append(dfDownRight, ignore_index = True).append(dfDownLeft, ignore_index = True)\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870639</td>\n",
       "      <td>0.778903</td>\n",
       "      <td>0.725278</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>0.661416</td>\n",
       "      <td>0.679687</td>\n",
       "      <td>0.725736</td>\n",
       "      <td>0.733129</td>\n",
       "      <td>0.697956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440451</td>\n",
       "      <td>0.461847</td>\n",
       "      <td>0.483469</td>\n",
       "      <td>0.492717</td>\n",
       "      <td>0.535039</td>\n",
       "      <td>0.545349</td>\n",
       "      <td>0.529865</td>\n",
       "      <td>0.501872</td>\n",
       "      <td>0.437781</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873671</td>\n",
       "      <td>0.781486</td>\n",
       "      <td>0.735074</td>\n",
       "      <td>0.683443</td>\n",
       "      <td>0.655249</td>\n",
       "      <td>0.673143</td>\n",
       "      <td>0.724177</td>\n",
       "      <td>0.729398</td>\n",
       "      <td>0.696364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465730</td>\n",
       "      <td>0.481957</td>\n",
       "      <td>0.495796</td>\n",
       "      <td>0.492139</td>\n",
       "      <td>0.528186</td>\n",
       "      <td>0.541326</td>\n",
       "      <td>0.529552</td>\n",
       "      <td>0.497978</td>\n",
       "      <td>0.431634</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877431</td>\n",
       "      <td>0.790890</td>\n",
       "      <td>0.744683</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>0.677805</td>\n",
       "      <td>0.718668</td>\n",
       "      <td>0.727266</td>\n",
       "      <td>0.696576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448371</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.488073</td>\n",
       "      <td>0.494257</td>\n",
       "      <td>0.521760</td>\n",
       "      <td>0.529427</td>\n",
       "      <td>0.515435</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.433165</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.792698</td>\n",
       "      <td>0.741344</td>\n",
       "      <td>0.697454</td>\n",
       "      <td>0.673473</td>\n",
       "      <td>0.686590</td>\n",
       "      <td>0.721158</td>\n",
       "      <td>0.718938</td>\n",
       "      <td>0.687107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426923</td>\n",
       "      <td>0.447417</td>\n",
       "      <td>0.465064</td>\n",
       "      <td>0.479192</td>\n",
       "      <td>0.519957</td>\n",
       "      <td>0.540319</td>\n",
       "      <td>0.528465</td>\n",
       "      <td>0.506383</td>\n",
       "      <td>0.439765</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.794662</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.701630</td>\n",
       "      <td>0.679918</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.731132</td>\n",
       "      <td>0.732642</td>\n",
       "      <td>0.692457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448210</td>\n",
       "      <td>0.467434</td>\n",
       "      <td>0.485004</td>\n",
       "      <td>0.482802</td>\n",
       "      <td>0.528204</td>\n",
       "      <td>0.543669</td>\n",
       "      <td>0.532557</td>\n",
       "      <td>0.512542</td>\n",
       "      <td>0.446091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6  \\\n",
       "8995  1.0  0.870639  0.778903  0.725278  0.680233  0.661416  0.679687   \n",
       "8996  1.0  0.873671  0.781486  0.735074  0.683443  0.655249  0.673143   \n",
       "8997  1.0  0.877431  0.790890  0.744683  0.692465  0.664408  0.677805   \n",
       "8998  1.0  0.881703  0.792698  0.741344  0.697454  0.673473  0.686590   \n",
       "8999  1.0  0.881703  0.794662  0.747706  0.701630  0.679918  0.693795   \n",
       "\n",
       "             7         8         9  ...        51        52        53  \\\n",
       "8995  0.725736  0.733129  0.697956  ...  0.440451  0.461847  0.483469   \n",
       "8996  0.724177  0.729398  0.696364  ...  0.465730  0.481957  0.495796   \n",
       "8997  0.718668  0.727266  0.696576  ...  0.448371  0.470152  0.488073   \n",
       "8998  0.721158  0.718938  0.687107  ...  0.426923  0.447417  0.465064   \n",
       "8999  0.731132  0.732642  0.692457  ...  0.448210  0.467434  0.485004   \n",
       "\n",
       "            54        55        56        57        58        59  RESULT  \n",
       "8995  0.492717  0.535039  0.545349  0.529865  0.501872  0.437781       8  \n",
       "8996  0.492139  0.528186  0.541326  0.529552  0.497978  0.431634       8  \n",
       "8997  0.494257  0.521760  0.529427  0.515435  0.493970  0.433165       8  \n",
       "8998  0.479192  0.519957  0.540319  0.528465  0.506383  0.439765       8  \n",
       "8999  0.482802  0.528204  0.543669  0.532557  0.512542  0.446091       8  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 61)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data and desired output, and then split training and test observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = df[columns]\n",
    "y = df['RESULT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=14)\n",
    "\n",
    "c = np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "\n",
    "n_classes = 9\n",
    "#one hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print (y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values \n",
    "Y_train_np = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X_train).astype('float32')\n",
    "Y = np.array(y_train)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 60)\n",
      "(1800, 9)\n",
      "(7200, 60, 1)\n",
      "(1800, 60, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7200, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train_np = np.expand_dims(X_train_np, axis=2)\n",
    "print(X_train_np.shape)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "print(X_test.shape)\n",
    "#Y_train_np = np.expand_dims(Y_train_np, axis=2)\n",
    "Y_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(layer_sizes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, padding ='same', input_shape=(60,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "  \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "  \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(lr=0.00125),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate(model, batch_size=128, epochs=100):\n",
    "    model.summary()\n",
    "    history = model.fit(X_train_np, Y_train_np, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 60, 32)            128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 60, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 30, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 30, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 30, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 15, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 15, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 45,321\n",
      "Trainable params: 45,001\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 2.3141 - acc: 0.0997 - val_loss: 2.2285 - val_acc: 0.1172\n",
      "Epoch 2/100\n",
      "7200/7200 [==============================] - 7s 918us/step - loss: 2.2200 - acc: 0.1304 - val_loss: 2.1526 - val_acc: 0.1161\n",
      "Epoch 3/100\n",
      "7200/7200 [==============================] - 7s 923us/step - loss: 2.1581 - acc: 0.1644 - val_loss: 2.0927 - val_acc: 0.2233\n",
      "Epoch 4/100\n",
      "7200/7200 [==============================] - 6s 899us/step - loss: 2.1000 - acc: 0.2071 - val_loss: 2.0394 - val_acc: 0.3256\n",
      "Epoch 5/100\n",
      "7200/7200 [==============================] - 7s 945us/step - loss: 2.0508 - acc: 0.2526 - val_loss: 1.9878 - val_acc: 0.3994\n",
      "Epoch 6/100\n",
      "7200/7200 [==============================] - 7s 903us/step - loss: 2.0056 - acc: 0.2999 - val_loss: 1.9350 - val_acc: 0.4756\n",
      "Epoch 7/100\n",
      "7200/7200 [==============================] - 7s 933us/step - loss: 1.9536 - acc: 0.3536 - val_loss: 1.8811 - val_acc: 0.4994\n",
      "Epoch 8/100\n",
      "7200/7200 [==============================] - 7s 919us/step - loss: 1.9049 - acc: 0.3922 - val_loss: 1.8285 - val_acc: 0.5417\n",
      "Epoch 9/100\n",
      "7200/7200 [==============================] - 7s 947us/step - loss: 1.8602 - acc: 0.4208 - val_loss: 1.7765 - val_acc: 0.5850\n",
      "Epoch 10/100\n",
      "7200/7200 [==============================] - 6s 896us/step - loss: 1.8129 - acc: 0.4483 - val_loss: 1.7233 - val_acc: 0.6383\n",
      "Epoch 11/100\n",
      "7200/7200 [==============================] - 6s 895us/step - loss: 1.7614 - acc: 0.4751 - val_loss: 1.6693 - val_acc: 0.6783\n",
      "Epoch 12/100\n",
      "7200/7200 [==============================] - 6s 896us/step - loss: 1.7080 - acc: 0.5008 - val_loss: 1.6167 - val_acc: 0.7078\n",
      "Epoch 13/100\n",
      "7200/7200 [==============================] - 6s 897us/step - loss: 1.6651 - acc: 0.5161 - val_loss: 1.5667 - val_acc: 0.7339\n",
      "Epoch 14/100\n",
      "7200/7200 [==============================] - 7s 932us/step - loss: 1.6215 - acc: 0.5387 - val_loss: 1.5177 - val_acc: 0.7539\n",
      "Epoch 15/100\n",
      "7200/7200 [==============================] - 7s 929us/step - loss: 1.5786 - acc: 0.5461 - val_loss: 1.4728 - val_acc: 0.7678\n",
      "Epoch 16/100\n",
      "7200/7200 [==============================] - 6s 894us/step - loss: 1.5279 - acc: 0.5700 - val_loss: 1.4249 - val_acc: 0.7800\n",
      "Epoch 17/100\n",
      "7200/7200 [==============================] - 6s 892us/step - loss: 1.4924 - acc: 0.5797 - val_loss: 1.3780 - val_acc: 0.7906\n",
      "Epoch 18/100\n",
      "7200/7200 [==============================] - 6s 897us/step - loss: 1.4552 - acc: 0.5922 - val_loss: 1.3339 - val_acc: 0.8011\n",
      "Epoch 19/100\n",
      "7200/7200 [==============================] - 7s 962us/step - loss: 1.4094 - acc: 0.6125 - val_loss: 1.2907 - val_acc: 0.8083\n",
      "Epoch 20/100\n",
      "7200/7200 [==============================] - 6s 898us/step - loss: 1.3780 - acc: 0.6257 - val_loss: 1.2478 - val_acc: 0.8106\n",
      "Epoch 21/100\n",
      "7200/7200 [==============================] - 7s 995us/step - loss: 1.3328 - acc: 0.6381 - val_loss: 1.2061 - val_acc: 0.8200\n",
      "Epoch 22/100\n",
      "7200/7200 [==============================] - 7s 911us/step - loss: 1.2923 - acc: 0.6660 - val_loss: 1.1643 - val_acc: 0.8217\n",
      "Epoch 23/100\n",
      "7200/7200 [==============================] - 7s 917us/step - loss: 1.2555 - acc: 0.6792 - val_loss: 1.1268 - val_acc: 0.8339\n",
      "Epoch 24/100\n",
      "7200/7200 [==============================] - 7s 931us/step - loss: 1.2152 - acc: 0.6913 - val_loss: 1.0882 - val_acc: 0.8383\n",
      "Epoch 25/100\n",
      "7200/7200 [==============================] - 6s 897us/step - loss: 1.1841 - acc: 0.6960 - val_loss: 1.0495 - val_acc: 0.8428\n",
      "Epoch 26/100\n",
      "7200/7200 [==============================] - 6s 837us/step - loss: 1.1390 - acc: 0.7211 - val_loss: 1.0115 - val_acc: 0.8478\n",
      "Epoch 27/100\n",
      "7200/7200 [==============================] - 6s 851us/step - loss: 1.1082 - acc: 0.7269 - val_loss: 0.9781 - val_acc: 0.8572\n",
      "Epoch 28/100\n",
      "7200/7200 [==============================] - 6s 842us/step - loss: 1.0790 - acc: 0.7386 - val_loss: 0.9414 - val_acc: 0.8611\n",
      "Epoch 29/100\n",
      "7200/7200 [==============================] - 6s 840us/step - loss: 1.0423 - acc: 0.7539 - val_loss: 0.9085 - val_acc: 0.8672\n",
      "Epoch 30/100\n",
      "7200/7200 [==============================] - 6s 889us/step - loss: 1.0198 - acc: 0.7615 - val_loss: 0.8744 - val_acc: 0.8772\n",
      "Epoch 31/100\n",
      "7200/7200 [==============================] - 7s 939us/step - loss: 0.9933 - acc: 0.7653 - val_loss: 0.8447 - val_acc: 0.8800\n",
      "Epoch 32/100\n",
      "7200/7200 [==============================] - 7s 910us/step - loss: 0.9577 - acc: 0.7722 - val_loss: 0.8169 - val_acc: 0.8806\n",
      "Epoch 33/100\n",
      "7200/7200 [==============================] - 7s 908us/step - loss: 0.9334 - acc: 0.7853 - val_loss: 0.7859 - val_acc: 0.8906\n",
      "Epoch 34/100\n",
      "7200/7200 [==============================] - 7s 906us/step - loss: 0.9016 - acc: 0.7931 - val_loss: 0.7563 - val_acc: 0.8983\n",
      "Epoch 35/100\n",
      "7200/7200 [==============================] - 7s 946us/step - loss: 0.8825 - acc: 0.8021 - val_loss: 0.7296 - val_acc: 0.9044\n",
      "Epoch 36/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.8558 - acc: 0.8093 - val_loss: 0.7054 - val_acc: 0.9100\n",
      "Epoch 37/100\n",
      "7200/7200 [==============================] - 7s 936us/step - loss: 0.8347 - acc: 0.8132 - val_loss: 0.6813 - val_acc: 0.9128\n",
      "Epoch 38/100\n",
      "7200/7200 [==============================] - 7s 915us/step - loss: 0.8094 - acc: 0.8239 - val_loss: 0.6589 - val_acc: 0.9150\n",
      "Epoch 39/100\n",
      "7200/7200 [==============================] - 7s 984us/step - loss: 0.7934 - acc: 0.8215 - val_loss: 0.6361 - val_acc: 0.9172\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 6s 890us/step - loss: 0.7709 - acc: 0.8279 - val_loss: 0.6143 - val_acc: 0.9272\n",
      "Epoch 41/100\n",
      "7200/7200 [==============================] - 7s 905us/step - loss: 0.7443 - acc: 0.8374 - val_loss: 0.5955 - val_acc: 0.9289\n",
      "Epoch 42/100\n",
      "7200/7200 [==============================] - 6s 884us/step - loss: 0.7246 - acc: 0.8428 - val_loss: 0.5781 - val_acc: 0.9289\n",
      "Epoch 43/100\n",
      "7200/7200 [==============================] - 7s 963us/step - loss: 0.7049 - acc: 0.8410 - val_loss: 0.5592 - val_acc: 0.9306\n",
      "Epoch 44/100\n",
      "7200/7200 [==============================] - 7s 913us/step - loss: 0.6998 - acc: 0.8494 - val_loss: 0.5413 - val_acc: 0.9339\n",
      "Epoch 45/100\n",
      "7200/7200 [==============================] - 6s 899us/step - loss: 0.6748 - acc: 0.8579 - val_loss: 0.5245 - val_acc: 0.9317\n",
      "Epoch 46/100\n",
      "7200/7200 [==============================] - 6s 894us/step - loss: 0.6626 - acc: 0.8571 - val_loss: 0.5084 - val_acc: 0.9328\n",
      "Epoch 47/100\n",
      "7200/7200 [==============================] - 7s 929us/step - loss: 0.6343 - acc: 0.8679 - val_loss: 0.4936 - val_acc: 0.9322\n",
      "Epoch 48/100\n",
      "7200/7200 [==============================] - 7s 958us/step - loss: 0.6302 - acc: 0.8689 - val_loss: 0.4781 - val_acc: 0.9372\n",
      "Epoch 49/100\n",
      "7200/7200 [==============================] - 7s 904us/step - loss: 0.6114 - acc: 0.8782 - val_loss: 0.4636 - val_acc: 0.9378\n",
      "Epoch 50/100\n",
      "7200/7200 [==============================] - 7s 914us/step - loss: 0.6011 - acc: 0.8792 - val_loss: 0.4527 - val_acc: 0.9378\n",
      "Epoch 51/100\n",
      "7200/7200 [==============================] - 7s 934us/step - loss: 0.5923 - acc: 0.8740 - val_loss: 0.4405 - val_acc: 0.9389\n",
      "Epoch 52/100\n",
      "7200/7200 [==============================] - 7s 936us/step - loss: 0.5761 - acc: 0.8861 - val_loss: 0.4268 - val_acc: 0.9389\n",
      "Epoch 53/100\n",
      "7200/7200 [==============================] - 7s 903us/step - loss: 0.5588 - acc: 0.8842 - val_loss: 0.4162 - val_acc: 0.9411\n",
      "Epoch 54/100\n",
      "7200/7200 [==============================] - 7s 904us/step - loss: 0.5508 - acc: 0.8871 - val_loss: 0.4078 - val_acc: 0.9411\n",
      "Epoch 55/100\n",
      "7200/7200 [==============================] - 7s 933us/step - loss: 0.5298 - acc: 0.8951 - val_loss: 0.3958 - val_acc: 0.9422\n",
      "Epoch 56/100\n",
      "7200/7200 [==============================] - 7s 904us/step - loss: 0.5236 - acc: 0.8969 - val_loss: 0.3853 - val_acc: 0.9417\n",
      "Epoch 57/100\n",
      "7200/7200 [==============================] - 7s 905us/step - loss: 0.5193 - acc: 0.8950 - val_loss: 0.3768 - val_acc: 0.9444\n",
      "Epoch 58/100\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.5077 - acc: 0.8950 - val_loss: 0.3677 - val_acc: 0.9444\n",
      "Epoch 59/100\n",
      "7200/7200 [==============================] - 7s 944us/step - loss: 0.5004 - acc: 0.9006 - val_loss: 0.3591 - val_acc: 0.9467\n",
      "Epoch 60/100\n",
      "7200/7200 [==============================] - 6s 902us/step - loss: 0.4897 - acc: 0.9046 - val_loss: 0.3488 - val_acc: 0.9467\n",
      "Epoch 61/100\n",
      "7200/7200 [==============================] - 7s 928us/step - loss: 0.4788 - acc: 0.9054 - val_loss: 0.3419 - val_acc: 0.9467\n",
      "Epoch 62/100\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.4751 - acc: 0.9058 - val_loss: 0.3353 - val_acc: 0.9467\n",
      "Epoch 63/100\n",
      "7200/7200 [==============================] - 7s 907us/step - loss: 0.4591 - acc: 0.9114 - val_loss: 0.3266 - val_acc: 0.9489\n",
      "Epoch 64/100\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.4571 - acc: 0.9078 - val_loss: 0.3199 - val_acc: 0.9489\n",
      "Epoch 65/100\n",
      "7200/7200 [==============================] - 7s 935us/step - loss: 0.4437 - acc: 0.9104 - val_loss: 0.3144 - val_acc: 0.9500\n",
      "Epoch 66/100\n",
      "7200/7200 [==============================] - 7s 933us/step - loss: 0.4447 - acc: 0.9074 - val_loss: 0.3094 - val_acc: 0.9500\n",
      "Epoch 67/100\n",
      "7200/7200 [==============================] - 6s 903us/step - loss: 0.4339 - acc: 0.9129 - val_loss: 0.3013 - val_acc: 0.9539\n",
      "Epoch 68/100\n",
      "7200/7200 [==============================] - 7s 907us/step - loss: 0.4244 - acc: 0.9182 - val_loss: 0.2959 - val_acc: 0.9533\n",
      "Epoch 69/100\n",
      "7200/7200 [==============================] - 7s 929us/step - loss: 0.4192 - acc: 0.9169 - val_loss: 0.2892 - val_acc: 0.9539\n",
      "Epoch 70/100\n",
      "7200/7200 [==============================] - 7s 952us/step - loss: 0.4140 - acc: 0.9187 - val_loss: 0.2852 - val_acc: 0.9539\n",
      "Epoch 71/100\n",
      "7200/7200 [==============================] - 7s 910us/step - loss: 0.4072 - acc: 0.9214 - val_loss: 0.2800 - val_acc: 0.9544\n",
      "Epoch 72/100\n",
      "7200/7200 [==============================] - 7s 968us/step - loss: 0.4007 - acc: 0.9200 - val_loss: 0.2753 - val_acc: 0.9550\n",
      "Epoch 73/100\n",
      "7200/7200 [==============================] - 7s 910us/step - loss: 0.3961 - acc: 0.9218 - val_loss: 0.2708 - val_acc: 0.9544\n",
      "Epoch 74/100\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3876 - acc: 0.9251 - val_loss: 0.2671 - val_acc: 0.9550\n",
      "Epoch 75/100\n",
      "7200/7200 [==============================] - 7s 921us/step - loss: 0.3791 - acc: 0.9235 - val_loss: 0.2643 - val_acc: 0.9556\n",
      "Epoch 76/100\n",
      "7200/7200 [==============================] - 7s 928us/step - loss: 0.3781 - acc: 0.9264 - val_loss: 0.2578 - val_acc: 0.9556\n",
      "Epoch 77/100\n",
      "7200/7200 [==============================] - 7s 931us/step - loss: 0.3698 - acc: 0.9275 - val_loss: 0.2529 - val_acc: 0.9556\n",
      "Epoch 78/100\n",
      "7200/7200 [==============================] - 6s 897us/step - loss: 0.3634 - acc: 0.9288 - val_loss: 0.2490 - val_acc: 0.9572\n",
      "Epoch 79/100\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.3625 - acc: 0.9279 - val_loss: 0.2459 - val_acc: 0.9572\n",
      "Epoch 80/100\n",
      "7200/7200 [==============================] - 6s 898us/step - loss: 0.3562 - acc: 0.9281 - val_loss: 0.2406 - val_acc: 0.9578\n",
      "Epoch 81/100\n",
      "7200/7200 [==============================] - 6s 900us/step - loss: 0.3512 - acc: 0.9304 - val_loss: 0.2377 - val_acc: 0.9572\n",
      "Epoch 82/100\n",
      "7200/7200 [==============================] - 6s 897us/step - loss: 0.3472 - acc: 0.9303 - val_loss: 0.2342 - val_acc: 0.9578\n",
      "Epoch 83/100\n",
      "7200/7200 [==============================] - 6s 898us/step - loss: 0.3464 - acc: 0.9276 - val_loss: 0.2320 - val_acc: 0.9578\n",
      "Epoch 84/100\n",
      "7200/7200 [==============================] - 7s 959us/step - loss: 0.3387 - acc: 0.9344 - val_loss: 0.2283 - val_acc: 0.9583\n",
      "Epoch 85/100\n",
      "7200/7200 [==============================] - 7s 911us/step - loss: 0.3363 - acc: 0.9368 - val_loss: 0.2245 - val_acc: 0.9589\n",
      "Epoch 86/100\n",
      "7200/7200 [==============================] - 7s 912us/step - loss: 0.3304 - acc: 0.9336 - val_loss: 0.2216 - val_acc: 0.9583\n",
      "Epoch 87/100\n",
      "7200/7200 [==============================] - 7s 987us/step - loss: 0.3250 - acc: 0.9378 - val_loss: 0.2190 - val_acc: 0.9594\n",
      "Epoch 88/100\n",
      "7200/7200 [==============================] - 7s 923us/step - loss: 0.3257 - acc: 0.9353 - val_loss: 0.2160 - val_acc: 0.9578\n",
      "Epoch 89/100\n",
      "7200/7200 [==============================] - 7s 904us/step - loss: 0.3254 - acc: 0.9329 - val_loss: 0.2131 - val_acc: 0.9594\n",
      "Epoch 90/100\n",
      "7200/7200 [==============================] - 7s 913us/step - loss: 0.3169 - acc: 0.9337 - val_loss: 0.2107 - val_acc: 0.9589\n",
      "Epoch 91/100\n",
      "7200/7200 [==============================] - 7s 918us/step - loss: 0.3180 - acc: 0.9353 - val_loss: 0.2081 - val_acc: 0.9578\n",
      "Epoch 92/100\n",
      "7200/7200 [==============================] - 7s 908us/step - loss: 0.3153 - acc: 0.9365 - val_loss: 0.2054 - val_acc: 0.9589\n",
      "Epoch 93/100\n",
      "7200/7200 [==============================] - 7s 909us/step - loss: 0.3084 - acc: 0.9356 - val_loss: 0.2030 - val_acc: 0.9589\n",
      "Epoch 94/100\n",
      "7200/7200 [==============================] - 7s 908us/step - loss: 0.3000 - acc: 0.9421 - val_loss: 0.2007 - val_acc: 0.9594\n",
      "Epoch 95/100\n",
      "7200/7200 [==============================] - 7s 926us/step - loss: 0.3028 - acc: 0.9415 - val_loss: 0.1982 - val_acc: 0.9606\n",
      "Epoch 96/100\n",
      "7200/7200 [==============================] - 7s 909us/step - loss: 0.3031 - acc: 0.9390 - val_loss: 0.1966 - val_acc: 0.9583\n",
      "Epoch 97/100\n",
      "7200/7200 [==============================] - 7s 907us/step - loss: 0.2938 - acc: 0.9387 - val_loss: 0.1933 - val_acc: 0.9606\n",
      "Epoch 98/100\n",
      "7200/7200 [==============================] - 7s 905us/step - loss: 0.2893 - acc: 0.9424 - val_loss: 0.1915 - val_acc: 0.9611\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 6s 883us/step - loss: 0.2842 - acc: 0.9440 - val_loss: 0.1904 - val_acc: 0.9594\n",
      "Epoch 100/100\n",
      "7200/7200 [==============================] - 6s 881us/step - loss: 0.2832 - acc: 0.9435 - val_loss: 0.1866 - val_acc: 0.9617\n",
      "Test loss: 0.187\n",
      "Test accuracy: 0.962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for layers in range(1, 5):\n",
    "model = create_dense([32] * 5)\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test, y_test))\n",
    "def evaluate(model, batch_size=128, epochs=5):\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['training', 'validation'], loc='best')\n",
    "# plt.show()\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in range(1, 5):\n",
    "    model = create_dense([32] * layers)\n",
    "    evaluate(model)\n",
    "\n",
    "# model.save('model/model_9_positions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
