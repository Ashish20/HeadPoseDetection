# Head Pose Detection
Head pose is a significant indicator of a person’s interest in performing a particular task. Estimating the head pose of a person is a crucial problem that can have various applications. Audience measurement is an application area where face tracking and analysis could automatically provide reliable answers about how consumers and users behave spontaneously in real environments. By knowing audience emotional reactions and engagement towards a show, event, speech, any product, content or campaigns, one can offer an improved experience and customized content addressing the audience’s interests and behaviour. Recognizing driver awareness is an important prerequisite for the design of advanced automotive safety systems. Since visual attention is constrained to a driver's field of view, knowing where a driver is looking provides useful cues about his activity and awareness of the environment.
<br></br>Head pose estimation has its applications in other sectors as well such as virtual reality, aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. We present a way to solve the problem by identifying 60 different points on face and by calculating the distances of these points which are plotted on face the angle of the head with respect to camera can be calculated and thus the pose is understood. By applying different types of neural networks, the goal is to attain the maximum accuracy possible in classifying the input in nine different poses namely top, right, left, bottom, center, top left, top right, bottom left and bottom right. To maintain the integrity of the results the input data is produced by taking different faces to generate the data.
# Neural Networks Model - 
a. Fully Connected Neural Network - <br></br>
   The first model which we started with is fully connected Neural Network. The model consists of 3 dense layers. The activation function used is Relu (Rectified Linear Unit). The results of the model were unstable and the maximum accuracy which could be achieved was 83 percent by increasing the epochs to 100.
   <br></br>
b. Convolutional Neural Network with ReLU - <br></br>
    In order to increase the accuracy, we made use of Convolutional Neural Network technique. A deep network like ResNet would not be a suitable network for our input is quite small (60 features for a data point). Hence taking inspiration from ResNet, we build a much shallow network which consists of three residual blocks (a residual block has a combination of Conv - ReLU - Conv layers). In addition to this, the residual block is coupled with max pooling to reduce computation cost by input downsampling and batch normalisation layer is added to reduce covariance shift and to introduce slight regularization. The use of global average pooling (GAP) instead of a long network of fully connected layers facilitates less computational power and better generalization performance. The GAP layer is followed by two dense layers with ReLU activation and an output layer with softmax activation as done in the first model. As the above layer took a longer time to train (18-20 s per epoch), a simplification was made to the above residual block. The second convolutional layer was removed and batch normalisation block was placed right after the first convolutional layer. Accuracy achieved was still the same as a single conv layer was still able to train the current data, while the training time dropped to about 7-8 seconds per epoch as there were less convolutional layers to train. The maximum accuracy achieved was 96 percent when trained for 100 epochs.
    <br></br>
c. Convolutional Neural Network with ELU - <br></br>
    For the third model, to further improve the accuracy, ELU is used instead of ReLU and batch normalisation in the original residual block. This modifications addresses two important issues. Firstly, with ELU as the activation, we can get rid of dying ReLU problem. A "dead" ReLU always outputs the same value for any input. Probably this is arrived at by learning a large negative bias term for its weights. In turn, that means that it takes no role in discriminating between inputs. Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ELU with a non-zero value for negative inputs are one attempt to address this issue and give a chance for the neuron to recover. Secondly, ReLUs are non-negative and can have a mean activations larger than zero whereas ELUs have negative values and can push mean activations to zero. Thus ELUs reduce bias shift which speeds up learning process. The time taken for training per epoch is 5-6 seconds and the accuracy achieved was 98 percent at 100 epochs.
    
