{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Head poses Model Training\n",
    "\n",
    "Neural Network model for a quick Head Pose estimation, using just facial points as input. The model was made for real-time control of a Pan-Tilt camera using face movements as the control signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies, set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(14)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Dropout, Activation, Conv2D, MaxPooling2D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras import regularizers \n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 61)\n"
     ]
    }
   ],
   "source": [
    "dfCenter = pd.read_csv('datasets/face_center.csv')\n",
    "dfCenter2 = pd.read_csv('datasets/face_center2.csv')\n",
    "dfCenter = dfCenter.append(dfCenter2, ignore_index = True)\n",
    "\n",
    "dfRight = pd.read_csv('datasets/face_right.csv')\n",
    "dfRight2 = pd.read_csv('datasets/face_right2.csv')\n",
    "dfRight = dfRight.append(dfRight2, ignore_index = True)\n",
    "\n",
    "dfLeft = pd.read_csv('datasets/face_left.csv')\n",
    "dfLeft2 = pd.read_csv('datasets/face_left2.csv')\n",
    "dfLeft = dfLeft.append(dfLeft2, ignore_index = True)\n",
    "\n",
    "dfUp = pd.read_csv('datasets/face_up.csv')\n",
    "dfUp2 = pd.read_csv('datasets/face_up2.csv')\n",
    "dfUp = dfUp.append(dfUp2, ignore_index = True)\n",
    "\n",
    "dfDown = pd.read_csv('datasets/face_down.csv')\n",
    "dfDown2 = pd.read_csv('datasets/face_down2.csv')\n",
    "dfDown = dfDown.append(dfDown2, ignore_index = True)\n",
    "\n",
    "dfUpRight = pd.read_csv('datasets/face_up_right.csv')\n",
    "dfUpRight2 = pd.read_csv('datasets/face_up_right2.csv')\n",
    "dfUpRight = dfUpRight.append(dfUpRight2, ignore_index = True)\n",
    "\n",
    "dfUpLeft = pd.read_csv('datasets/face_up_left.csv')\n",
    "dfUpLeft2 = pd.read_csv('datasets/face_up_left2.csv')\n",
    "dfUpLeft = dfUpLeft.append(dfUpLeft2, ignore_index = True)\n",
    "\n",
    "dfDownRight = pd.read_csv('datasets/face_down_right.csv')\n",
    "dfDownRight2 = pd.read_csv('datasets/face_down_right2.csv')\n",
    "dfDownRight = dfDownRight.append(dfDownRight2, ignore_index = True)\n",
    "\n",
    "dfDownLeft = pd.read_csv('datasets/face_down_left.csv')\n",
    "dfDownLeft2 = pd.read_csv('datasets/face_down_left2.csv')\n",
    "dfDownLeft = dfDownLeft.append(dfDownLeft2, ignore_index = True)\n",
    "\n",
    "columns = list(dfCenter)\n",
    "\n",
    "dfCenter['RESULT'] = 0\n",
    "dfRight['RESULT'] = 1\n",
    "dfLeft['RESULT'] = 2\n",
    "dfUp['RESULT'] = 3\n",
    "dfDown['RESULT'] = 4\n",
    "dfUpRight['RESULT'] = 5\n",
    "dfUpLeft['RESULT'] = 6\n",
    "dfDownRight['RESULT'] = 7\n",
    "dfDownLeft['RESULT'] = 8\n",
    "\n",
    "\n",
    "df = dfCenter.append(dfRight, ignore_index=True).append(dfLeft, ignore_index = True)\n",
    "df = df.append(dfUp, ignore_index = True).append(dfDown, ignore_index = True)\n",
    "df = df.append(dfUpRight, ignore_index = True).append(dfUpLeft, ignore_index = True)\n",
    "df = df.append(dfDownRight, ignore_index = True).append(dfDownLeft, ignore_index = True)\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870639</td>\n",
       "      <td>0.778903</td>\n",
       "      <td>0.725278</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>0.661416</td>\n",
       "      <td>0.679687</td>\n",
       "      <td>0.725736</td>\n",
       "      <td>0.733129</td>\n",
       "      <td>0.697956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440451</td>\n",
       "      <td>0.461847</td>\n",
       "      <td>0.483469</td>\n",
       "      <td>0.492717</td>\n",
       "      <td>0.535039</td>\n",
       "      <td>0.545349</td>\n",
       "      <td>0.529865</td>\n",
       "      <td>0.501872</td>\n",
       "      <td>0.437781</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873671</td>\n",
       "      <td>0.781486</td>\n",
       "      <td>0.735074</td>\n",
       "      <td>0.683443</td>\n",
       "      <td>0.655249</td>\n",
       "      <td>0.673143</td>\n",
       "      <td>0.724177</td>\n",
       "      <td>0.729398</td>\n",
       "      <td>0.696364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465730</td>\n",
       "      <td>0.481957</td>\n",
       "      <td>0.495796</td>\n",
       "      <td>0.492139</td>\n",
       "      <td>0.528186</td>\n",
       "      <td>0.541326</td>\n",
       "      <td>0.529552</td>\n",
       "      <td>0.497978</td>\n",
       "      <td>0.431634</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877431</td>\n",
       "      <td>0.790890</td>\n",
       "      <td>0.744683</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>0.677805</td>\n",
       "      <td>0.718668</td>\n",
       "      <td>0.727266</td>\n",
       "      <td>0.696576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448371</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.488073</td>\n",
       "      <td>0.494257</td>\n",
       "      <td>0.521760</td>\n",
       "      <td>0.529427</td>\n",
       "      <td>0.515435</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.433165</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.792698</td>\n",
       "      <td>0.741344</td>\n",
       "      <td>0.697454</td>\n",
       "      <td>0.673473</td>\n",
       "      <td>0.686590</td>\n",
       "      <td>0.721158</td>\n",
       "      <td>0.718938</td>\n",
       "      <td>0.687107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426923</td>\n",
       "      <td>0.447417</td>\n",
       "      <td>0.465064</td>\n",
       "      <td>0.479192</td>\n",
       "      <td>0.519957</td>\n",
       "      <td>0.540319</td>\n",
       "      <td>0.528465</td>\n",
       "      <td>0.506383</td>\n",
       "      <td>0.439765</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.881703</td>\n",
       "      <td>0.794662</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.701630</td>\n",
       "      <td>0.679918</td>\n",
       "      <td>0.693795</td>\n",
       "      <td>0.731132</td>\n",
       "      <td>0.732642</td>\n",
       "      <td>0.692457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448210</td>\n",
       "      <td>0.467434</td>\n",
       "      <td>0.485004</td>\n",
       "      <td>0.482802</td>\n",
       "      <td>0.528204</td>\n",
       "      <td>0.543669</td>\n",
       "      <td>0.532557</td>\n",
       "      <td>0.512542</td>\n",
       "      <td>0.446091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6  \\\n",
       "8995  1.0  0.870639  0.778903  0.725278  0.680233  0.661416  0.679687   \n",
       "8996  1.0  0.873671  0.781486  0.735074  0.683443  0.655249  0.673143   \n",
       "8997  1.0  0.877431  0.790890  0.744683  0.692465  0.664408  0.677805   \n",
       "8998  1.0  0.881703  0.792698  0.741344  0.697454  0.673473  0.686590   \n",
       "8999  1.0  0.881703  0.794662  0.747706  0.701630  0.679918  0.693795   \n",
       "\n",
       "             7         8         9  ...        51        52        53  \\\n",
       "8995  0.725736  0.733129  0.697956  ...  0.440451  0.461847  0.483469   \n",
       "8996  0.724177  0.729398  0.696364  ...  0.465730  0.481957  0.495796   \n",
       "8997  0.718668  0.727266  0.696576  ...  0.448371  0.470152  0.488073   \n",
       "8998  0.721158  0.718938  0.687107  ...  0.426923  0.447417  0.465064   \n",
       "8999  0.731132  0.732642  0.692457  ...  0.448210  0.467434  0.485004   \n",
       "\n",
       "            54        55        56        57        58        59  RESULT  \n",
       "8995  0.492717  0.535039  0.545349  0.529865  0.501872  0.437781       8  \n",
       "8996  0.492139  0.528186  0.541326  0.529552  0.497978  0.431634       8  \n",
       "8997  0.494257  0.521760  0.529427  0.515435  0.493970  0.433165       8  \n",
       "8998  0.479192  0.519957  0.540319  0.528465  0.506383  0.439765       8  \n",
       "8999  0.482802  0.528204  0.543669  0.532557  0.512542  0.446091       8  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data and desired output, and then split training and test observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = df[columns]\n",
    "y = df['RESULT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=14)\n",
    "\n",
    "c = np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "\n",
    "n_classes = 9\n",
    "#one hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print (y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.values \n",
    "Y_train_np = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X_train).astype('float32')\n",
    "Y = np.array(y_train)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 60)\n",
      "(1800, 9)\n",
      "(7200, 60, 1)\n",
      "(1800, 60, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7200, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train_np = np.expand_dims(X_train_np, axis=2)\n",
    "print(X_train_np.shape)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "print(X_test.shape)\n",
    "#Y_train_np = np.expand_dims(Y_train_np, axis=2)\n",
    "Y_train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense(layer_sizes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(32, 3, padding ='same', input_shape=(60,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(32, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding ='same'))\n",
    "    model.add(BatchNormalization())\n",
    "  \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "  \n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(128, activation = 'elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(lr=0.00125),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate(model, batch_size=128, epochs=100):\n",
    "    model.summary()\n",
    "    history = model.fit(X_train_np, Y_train_np, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 60, 32)            128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 60, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 60, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 60, 32)            3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 60, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 60, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 60, 64)            256       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 73,769\n",
      "Trainable params: 73,129\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      "7200/7200 [==============================] - 25s 3ms/step - loss: 2.0689 - acc: 0.2999 - val_loss: 1.9420 - val_acc: 0.4161\n",
      "Epoch 2/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.8858 - acc: 0.4332 - val_loss: 1.7842 - val_acc: 0.4544\n",
      "Epoch 3/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.7465 - acc: 0.5086 - val_loss: 1.6563 - val_acc: 0.4800\n",
      "Epoch 4/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.6328 - acc: 0.5750 - val_loss: 1.5495 - val_acc: 0.5039\n",
      "Epoch 5/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.5335 - acc: 0.6265 - val_loss: 1.4609 - val_acc: 0.5400\n",
      "Epoch 6/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.4454 - acc: 0.6743 - val_loss: 1.3836 - val_acc: 0.6133\n",
      "Epoch 7/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.3661 - acc: 0.7217 - val_loss: 1.3099 - val_acc: 0.6906\n",
      "Epoch 8/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 1.2873 - acc: 0.7519 - val_loss: 1.2449 - val_acc: 0.7378\n",
      "Epoch 9/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.2178 - acc: 0.7811 - val_loss: 1.1742 - val_acc: 0.7900\n",
      "Epoch 10/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.1517 - acc: 0.7988 - val_loss: 1.1161 - val_acc: 0.8156\n",
      "Epoch 11/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 1.0920 - acc: 0.8038 - val_loss: 1.0469 - val_acc: 0.8400\n",
      "Epoch 12/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 1.0322 - acc: 0.8174 - val_loss: 0.9918 - val_acc: 0.8456\n",
      "Epoch 13/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.9847 - acc: 0.8300 - val_loss: 0.9405 - val_acc: 0.8506\n",
      "Epoch 14/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.9416 - acc: 0.8299 - val_loss: 0.8870 - val_acc: 0.8578\n",
      "Epoch 15/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.8929 - acc: 0.8386 - val_loss: 0.8484 - val_acc: 0.8567\n",
      "Epoch 16/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.8586 - acc: 0.8374 - val_loss: 0.8020 - val_acc: 0.8611\n",
      "Epoch 17/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.8198 - acc: 0.8451 - val_loss: 0.7705 - val_acc: 0.8617\n",
      "Epoch 18/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.7881 - acc: 0.8475 - val_loss: 0.7363 - val_acc: 0.8622\n",
      "Epoch 19/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.7609 - acc: 0.8518 - val_loss: 0.7111 - val_acc: 0.8639\n",
      "Epoch 20/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.7327 - acc: 0.8561 - val_loss: 0.6802 - val_acc: 0.8683\n",
      "Epoch 21/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.7101 - acc: 0.8614 - val_loss: 0.6598 - val_acc: 0.8672\n",
      "Epoch 22/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.6883 - acc: 0.8639 - val_loss: 0.6373 - val_acc: 0.8744\n",
      "Epoch 23/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.6636 - acc: 0.8694 - val_loss: 0.6174 - val_acc: 0.8772\n",
      "Epoch 24/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.6432 - acc: 0.8731 - val_loss: 0.5982 - val_acc: 0.8789\n",
      "Epoch 25/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.6246 - acc: 0.8739 - val_loss: 0.5789 - val_acc: 0.8783\n",
      "Epoch 26/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.6047 - acc: 0.8821 - val_loss: 0.5625 - val_acc: 0.8878\n",
      "Epoch 27/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.5948 - acc: 0.8807 - val_loss: 0.5499 - val_acc: 0.8867\n",
      "Epoch 28/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.5751 - acc: 0.8788 - val_loss: 0.5309 - val_acc: 0.8906\n",
      "Epoch 29/100\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.5646 - acc: 0.8867 - val_loss: 0.5160 - val_acc: 0.8983\n",
      "Epoch 30/100\n",
      "7200/7200 [==============================] - 23s 3ms/step - loss: 0.5487 - acc: 0.8872 - val_loss: 0.5035 - val_acc: 0.8944\n",
      "Epoch 31/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.5364 - acc: 0.8906 - val_loss: 0.4915 - val_acc: 0.8994\n",
      "Epoch 32/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.5198 - acc: 0.8949 - val_loss: 0.4788 - val_acc: 0.9033\n",
      "Epoch 33/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.5066 - acc: 0.8962 - val_loss: 0.4685 - val_acc: 0.9089\n",
      "Epoch 34/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.4985 - acc: 0.9026 - val_loss: 0.4546 - val_acc: 0.9061\n",
      "Epoch 35/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.4872 - acc: 0.9010 - val_loss: 0.4462 - val_acc: 0.9133\n",
      "Epoch 36/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.4728 - acc: 0.9026 - val_loss: 0.4347 - val_acc: 0.9189\n",
      "Epoch 37/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4637 - acc: 0.9064 - val_loss: 0.4237 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.4586 - acc: 0.9075 - val_loss: 0.4193 - val_acc: 0.9239\n",
      "Epoch 39/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4444 - acc: 0.9131 - val_loss: 0.4043 - val_acc: 0.9228\n",
      "Epoch 40/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4370 - acc: 0.9132 - val_loss: 0.3952 - val_acc: 0.9289\n",
      "Epoch 41/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.4252 - acc: 0.9161 - val_loss: 0.3859 - val_acc: 0.9267\n",
      "Epoch 42/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4168 - acc: 0.9167 - val_loss: 0.3773 - val_acc: 0.9278\n",
      "Epoch 43/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4028 - acc: 0.9192 - val_loss: 0.3698 - val_acc: 0.9317\n",
      "Epoch 44/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.4043 - acc: 0.9189 - val_loss: 0.3629 - val_acc: 0.9272\n",
      "Epoch 45/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.3912 - acc: 0.9228 - val_loss: 0.3528 - val_acc: 0.9322\n",
      "Epoch 46/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3876 - acc: 0.9211 - val_loss: 0.3474 - val_acc: 0.9333\n",
      "Epoch 47/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.3762 - acc: 0.9250 - val_loss: 0.3456 - val_acc: 0.9350\n",
      "Epoch 48/100\n",
      "7200/7200 [==============================] - 20s 3ms/step - loss: 0.3760 - acc: 0.9260 - val_loss: 0.3331 - val_acc: 0.9356\n",
      "Epoch 49/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3652 - acc: 0.9279 - val_loss: 0.3249 - val_acc: 0.9317\n",
      "Epoch 50/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.3566 - acc: 0.9292 - val_loss: 0.3211 - val_acc: 0.9333\n",
      "Epoch 51/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.3525 - acc: 0.9308 - val_loss: 0.3200 - val_acc: 0.9328\n",
      "Epoch 52/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.3432 - acc: 0.9307 - val_loss: 0.3073 - val_acc: 0.9383\n",
      "Epoch 53/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.3441 - acc: 0.9300 - val_loss: 0.3050 - val_acc: 0.9394\n",
      "Epoch 54/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.3357 - acc: 0.9342 - val_loss: 0.2988 - val_acc: 0.9394\n",
      "Epoch 55/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.3265 - acc: 0.9337 - val_loss: 0.2912 - val_acc: 0.9361\n",
      "Epoch 56/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.3241 - acc: 0.9303 - val_loss: 0.2880 - val_acc: 0.9389\n",
      "Epoch 57/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3233 - acc: 0.9325 - val_loss: 0.2825 - val_acc: 0.9383\n",
      "Epoch 58/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.3132 - acc: 0.9360 - val_loss: 0.2772 - val_acc: 0.9400\n",
      "Epoch 59/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.3099 - acc: 0.9347 - val_loss: 0.2728 - val_acc: 0.9444\n",
      "Epoch 60/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.3018 - acc: 0.9376 - val_loss: 0.2834 - val_acc: 0.9372\n",
      "Epoch 61/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2995 - acc: 0.9379 - val_loss: 0.2654 - val_acc: 0.9422\n",
      "Epoch 62/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2941 - acc: 0.9394 - val_loss: 0.2656 - val_acc: 0.9417\n",
      "Epoch 63/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2899 - acc: 0.9394 - val_loss: 0.2581 - val_acc: 0.9433\n",
      "Epoch 64/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2870 - acc: 0.9390 - val_loss: 0.2521 - val_acc: 0.9467\n",
      "Epoch 65/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2784 - acc: 0.9413 - val_loss: 0.3110 - val_acc: 0.9367\n",
      "Epoch 66/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2770 - acc: 0.9426 - val_loss: 0.2433 - val_acc: 0.9483\n",
      "Epoch 67/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2764 - acc: 0.9413 - val_loss: 0.2400 - val_acc: 0.9472\n",
      "Epoch 68/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2685 - acc: 0.9447 - val_loss: 0.2352 - val_acc: 0.9500\n",
      "Epoch 69/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2673 - acc: 0.9439 - val_loss: 0.2429 - val_acc: 0.9450\n",
      "Epoch 70/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2648 - acc: 0.9437 - val_loss: 0.2295 - val_acc: 0.9494\n",
      "Epoch 71/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2563 - acc: 0.9449 - val_loss: 0.2336 - val_acc: 0.9500\n",
      "Epoch 72/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2590 - acc: 0.9432 - val_loss: 0.2471 - val_acc: 0.9444\n",
      "Epoch 73/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2505 - acc: 0.9468 - val_loss: 0.2348 - val_acc: 0.9489\n",
      "Epoch 74/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2538 - acc: 0.9471 - val_loss: 0.2323 - val_acc: 0.9456\n",
      "Epoch 75/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2463 - acc: 0.9474 - val_loss: 0.2180 - val_acc: 0.9494\n",
      "Epoch 76/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2429 - acc: 0.9471 - val_loss: 0.2155 - val_acc: 0.9494\n",
      "Epoch 77/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2405 - acc: 0.9462 - val_loss: 0.2200 - val_acc: 0.9478\n",
      "Epoch 78/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.2380 - acc: 0.9475 - val_loss: 0.2067 - val_acc: 0.9544\n",
      "Epoch 79/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2348 - acc: 0.9482 - val_loss: 0.2038 - val_acc: 0.9567\n",
      "Epoch 80/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2328 - acc: 0.9481 - val_loss: 0.2066 - val_acc: 0.9544\n",
      "Epoch 81/100\n",
      "7200/7200 [==============================] - 19s 3ms/step - loss: 0.2264 - acc: 0.9501 - val_loss: 0.2000 - val_acc: 0.9539\n",
      "Epoch 82/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2250 - acc: 0.9514 - val_loss: 0.2071 - val_acc: 0.9500\n",
      "Epoch 83/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2206 - acc: 0.9517 - val_loss: 0.2010 - val_acc: 0.9561\n",
      "Epoch 84/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2205 - acc: 0.9525 - val_loss: 0.1936 - val_acc: 0.9589\n",
      "Epoch 85/100\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.2190 - acc: 0.9508 - val_loss: 0.1905 - val_acc: 0.9572\n",
      "Epoch 86/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2147 - acc: 0.9536 - val_loss: 0.2146 - val_acc: 0.9528\n",
      "Epoch 87/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2138 - acc: 0.9512 - val_loss: 0.2061 - val_acc: 0.9489\n",
      "Epoch 88/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2075 - acc: 0.9540 - val_loss: 0.2005 - val_acc: 0.9567\n",
      "Epoch 89/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2067 - acc: 0.9549 - val_loss: 0.1833 - val_acc: 0.9583\n",
      "Epoch 90/100\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.2056 - acc: 0.9539 - val_loss: 0.1894 - val_acc: 0.9567\n",
      "Epoch 91/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.2004 - acc: 0.9543 - val_loss: 0.1897 - val_acc: 0.9544\n",
      "Epoch 92/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.2022 - acc: 0.9539 - val_loss: 0.1823 - val_acc: 0.9578\n",
      "Epoch 93/100\n",
      "7200/7200 [==============================] - 17s 2ms/step - loss: 0.1971 - acc: 0.9568 - val_loss: 0.1951 - val_acc: 0.9561\n",
      "Epoch 94/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.1924 - acc: 0.9561 - val_loss: 0.2481 - val_acc: 0.9328\n",
      "Epoch 95/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.1960 - acc: 0.9544 - val_loss: 0.1997 - val_acc: 0.9556\n",
      "Epoch 96/100\n",
      "7200/7200 [==============================] - 16s 2ms/step - loss: 0.1916 - acc: 0.9561 - val_loss: 0.1676 - val_acc: 0.9622\n",
      "Epoch 97/100\n",
      "4864/7200 [===================>..........] - ETA: 4s - loss: 0.1874 - acc: 0.9568"
     ]
    }
   ],
   "source": [
    "# for layers in range(1, 5):\n",
    "model = create_dense([32] * 5)\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test, y_test))\n",
    "def evaluate(model, batch_size=128, epochs=5):\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.00125), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    loss, accuracy  = model.evaluate(X_test, y_test, verbose=False)\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['training', 'validation'], loc='best')\n",
    "# plt.show()\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in range(1, 5):\n",
    "    model = create_dense([32] * layers)\n",
    "    evaluate(model)\n",
    "\n",
    "# model.save('model/model_9_positions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
